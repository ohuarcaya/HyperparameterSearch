{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# CAPA DE RED\n",
    "class neural_layer():\n",
    "    def __init__(self, n_conexiones, n_neuronas, f_activacion):\n",
    "        self.f_activacion =  f_activacion\n",
    "        self.bias = np.random.rand(1, n_neuronas) * 2 - 1\n",
    "        self.w = np.random.rand(n_conexiones, n_neuronas) * 2 - 1\n",
    "\n",
    "# función y derivada\n",
    "f_sigmoide = (lambda x: 1/(1 + np.exp(-x)), lambda x: x * (1 - x))\n",
    "f_relu = lambda x: np.maximum(0, x)\n",
    "f_tanh = lambda x: (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "\n",
    "df_  = pd.read_csv(\"data/UJIndoorLoc_trainingData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "19937/19937 [==============================] - 8s 378us/step - loss: 23666958990679.5586 - mean_squared_error: 23666958990679.5586 - mean_absolute_error: 4864869.8769\n",
      "Epoch 2/15\n",
      "19937/19937 [==============================] - 7s 354us/step - loss: 23666958972692.2500 - mean_squared_error: 23666958972692.2500 - mean_absolute_error: 4864869.8835\n",
      "Epoch 3/15\n",
      "19937/19937 [==============================] - 7s 361us/step - loss: 23666959035700.4297 - mean_squared_error: 23666959035700.4297 - mean_absolute_error: 4864869.8725\n",
      "Epoch 4/15\n",
      "19937/19937 [==============================] - 9s 429us/step - loss: 23666959072411.3672 - mean_squared_error: 23666959072411.3672 - mean_absolute_error: 4864869.8785\n",
      "Epoch 5/15\n",
      "19937/19937 [==============================] - 8s 379us/step - loss: 23666959075251.4727 - mean_squared_error: 23666959075251.4727 - mean_absolute_error: 4864869.8872\n",
      "Epoch 6/15\n",
      "19937/19937 [==============================] - 7s 373us/step - loss: 23666959019080.5742 - mean_squared_error: 23666959019080.5742 - mean_absolute_error: 4864869.8817\n",
      "Epoch 7/15\n",
      "19937/19937 [==============================] - 8s 402us/step - loss: 23666958973218.1953 - mean_squared_error: 23666958973218.1953 - mean_absolute_error: 4864869.8832\n",
      "Epoch 8/15\n",
      "19937/19937 [==============================] - 8s 425us/step - loss: 23666959048533.4766 - mean_squared_error: 23666959048533.4766 - mean_absolute_error: 4864869.8900\n",
      "Epoch 9/15\n",
      "19937/19937 [==============================] - 8s 382us/step - loss: 23666958960595.5195 - mean_squared_error: 23666958960595.5195 - mean_absolute_error: 4864869.8818\n",
      "Epoch 10/15\n",
      "19937/19937 [==============================] - 8s 400us/step - loss: 23666959031282.4922 - mean_squared_error: 23666959031282.4922 - mean_absolute_error: 4864869.8832\n",
      "Epoch 11/15\n",
      "19937/19937 [==============================] - 9s 433us/step - loss: 23666959051163.1992 - mean_squared_error: 23666959051163.1992 - mean_absolute_error: 4864869.8901\n",
      "Epoch 12/15\n",
      "19937/19937 [==============================] - 10s 480us/step - loss: 23666959028232.0117 - mean_squared_error: 23666959028232.0117 - mean_absolute_error: 4864869.8574\n",
      "Epoch 13/15\n",
      "19937/19937 [==============================] - 8s 412us/step - loss: 23666959072621.7500 - mean_squared_error: 23666959072621.7500 - mean_absolute_error: 4864869.8779\n",
      "Epoch 14/15\n",
      "19937/19937 [==============================] - 9s 442us/step - loss: 23666958991941.8242 - mean_squared_error: 23666958991941.8242 - mean_absolute_error: 4864869.8798\n",
      "Epoch 15/15\n",
      "19937/19937 [==============================] - 8s 385us/step - loss: 23666959093343.9688 - mean_squared_error: 23666959093343.9688 - mean_absolute_error: 4864869.8743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x132988c50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wifi_columns = 520\n",
    "x_train = np.array(df_[df_.columns[:520]])\n",
    "y_train = np.array(df_.LATITUDE)[:, np.newaxis]\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "\n",
    "f_activation = ['relu', 'sigmoid', 'tanh']\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(700, input_dim=wifi_columns, activation='relu'))\n",
    "model.add(Dense(500, input_dim=wifi_columns, activation='softsign'))\n",
    "model.add(Dense(350, input_dim=wifi_columns, activation='tanh'))\n",
    "model.add(Dense(180, input_dim=wifi_columns, activation='linear'))\n",
    "model.add(Dense(50, input_dim=wifi_columns, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['mse', 'mae'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=15)\n",
    "\n",
    "# evaluamos el modelo\n",
    "#scores = model.evaluate(training_data, target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown activation function:prelu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-e9ab23bb7ab8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#model.add(Dense(180, input_dim=wifi_columns, activation='linear'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwifi_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softsign'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'prelu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#relu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m model.compile(loss='mean_squared_error',\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/activations.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0midentifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/activations.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(name, custom_objects)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         printable_module_name='activation function')\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[0;32m--> 165\u001b[0;31m                                  ':' + function_name)\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown activation function:prelu"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def clean_dataframe(df, input_columns, target_column, list_val):\n",
    "    x_train = df[input_columns]\n",
    "    y_train = np.array(df[target_column])[:, np.newaxis] #y_train = df_train[target_column]\n",
    "    #if (list_val[0]):\n",
    "    x_train = x_train.apply(lambda x: 100-x, axis=1)\n",
    "    #if (list_val[1]):\n",
    "    #x_train = preprocessing.scale(x_train)\n",
    "    #if (list_val[2]):\n",
    "    x_train = preprocessing.normalize(x_train)\n",
    "    return np.array(x_train), y_train\n",
    "\n",
    "wifi_columns = 520+1\n",
    "#x_train = np.array(df_[df_.columns[:520]])\n",
    "#y_train = np.array(df_.LATITUDE)[:, np.newaxis]\n",
    "#x_train, y_train = clean_dataframe(df_, df_.columns[:520], \"LATITUDE\", [])\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "\n",
    "f_activation = ['relu', 'sigmoid', 'tanh']\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Dense(700, input_dim=wifi_columns, activation='sigmoid'))\n",
    "#model.add(Dense(500, input_dim=wifi_columns, activation='softsign'))\n",
    "#model.add(Dense(350, input_dim=wifi_columns, activation='tanh'))\n",
    "#model.add(Dense(180, input_dim=wifi_columns, activation='linear'))\n",
    "model.add(Dense(50, input_dim=wifi_columns, activation='tanh'))\n",
    "model.add(Dense(1, activation='linear'))#relu\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['mse', 'mae'])\n",
    "\n",
    "model.fit(x_train_normal, y_train, epochs=5)\n",
    "\n",
    "# evaluamos el modelo\n",
    "#scores = model.evaluate(training_data, target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAP001</th>\n",
       "      <th>WAP002</th>\n",
       "      <th>WAP003</th>\n",
       "      <th>WAP004</th>\n",
       "      <th>WAP005</th>\n",
       "      <th>WAP006</th>\n",
       "      <th>WAP007</th>\n",
       "      <th>WAP008</th>\n",
       "      <th>WAP009</th>\n",
       "      <th>WAP010</th>\n",
       "      <th>...</th>\n",
       "      <th>WAP511</th>\n",
       "      <th>WAP512</th>\n",
       "      <th>WAP513</th>\n",
       "      <th>WAP514</th>\n",
       "      <th>WAP515</th>\n",
       "      <th>WAP516</th>\n",
       "      <th>WAP517</th>\n",
       "      <th>WAP518</th>\n",
       "      <th>WAP519</th>\n",
       "      <th>WAP520</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>-97</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 520 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
       "0     100     100     100     100     100     100     100     100     100   \n",
       "1     100     100     100     100     100     100     100     100     100   \n",
       "2     100     100     100     100     100     100     100     -97     100   \n",
       "3     100     100     100     100     100     100     100     100     100   \n",
       "4     100     100     100     100     100     100     100     100     100   \n",
       "\n",
       "   WAP010   ...    WAP511  WAP512  WAP513  WAP514  WAP515  WAP516  WAP517  \\\n",
       "0     100   ...       100     100     100     100     100     100     100   \n",
       "1     100   ...       100     100     100     100     100     100     100   \n",
       "2     100   ...       100     100     100     100     100     100     100   \n",
       "3     100   ...       100     100     100     100     100     100     100   \n",
       "4     100   ...       100     100     100     100     100     100     100   \n",
       "\n",
       "   WAP518  WAP519  WAP520  \n",
       "0     100     100     100  \n",
       "1     100     100     100  \n",
       "2     100     100     100  \n",
       "3     100     100     100  \n",
       "4     100     100     100  \n",
       "\n",
       "[5 rows x 520 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.max(y_train)\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sigmoid:  $f(x) = \\frac{1}{1-e^{-x}}$\n",
    "# tanh: $f(x)=\\frac{2}{1+e^{-2x}}-1 = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$\n",
    "# relu: $f(x)=max(0, x)= \\lbrace^{0;\\ x\\lt 0}_{x;\\ x \\geq 0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import seaborn as sns\n",
    "sigm = lambda x: 1/(1 + np.e ** (-x))\n",
    "_x = np.linspace(-5,5, 100)\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.plot(_x, f_relu(_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_time        0.32818429470062255 +/- 0.04760400573155117\n",
       "score_time    0.013472676277160645 +/- 0.003568309638817945\n",
       "test_mae           89996.15594150961 +/- 130219.04564148556\n",
       "test_mse           50182173917689.72 +/- 120859445981983.53\n",
       "test_r2           -72293313736980.64 +/- 174112222814112.25\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fit_time      0.27582075595855715 +/- 0.008178107775353525\n",
       "score_time    0.012851715087890625 +/- 0.00144373510453069\n",
       "test_mae           85089.94062445458 +/- 127147.8782194712\n",
       "test_mse          62943139816736.89 +/- 171696184842814.25\n",
       "test_r2           -90676983460813.39 +/- 247348514208326.1\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fit_time              0.393526029586792 +/- 0.04435700880176216\n",
       "score_time      0.004812550544738769 +/- 5.6144529679850805e-05\n",
       "test_mae               25054195071.745655 +/- 22312965052.83703\n",
       "test_mse        8.774627572845523e+23 +/- 9.725739768424016e+23\n",
       "test_r2       -1.2640881303575365e+24 +/- 1.4011070097444798...\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fit_time            0.40135142803192136 +/- 0.02450366538910096\n",
       "score_time       0.005463814735412598 +/- 0.0013138840883400386\n",
       "test_mae              39703681649.503044 +/- 47188199814.019424\n",
       "test_mse      2.4123455799089627e+24 +/- 4.7506878187679806e+24\n",
       "test_r2       -3.4752670567128053e+24 +/- 6.843923611439712e+24\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fit_time            0.38530113697052004 +/- 0.03090588967060711\n",
       "score_time       0.004860925674438477 +/- 0.0002351358384326568\n",
       "test_mae                   5287487.811167 +/- 4331192.398147694\n",
       "test_mse      2.4542314656948216e+16 +/- 2.3220886580008628e+16\n",
       "test_r2         -3.535608593275871e+16 +/- 3.34524136306432e+16\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fit_time          0.4218210935592651 +/- 0.03143776730104532\n",
       "score_time    0.005525732040405273 +/- 0.0016193906762142043\n",
       "test_mae            71714.94019159491 +/- 188714.88522986489\n",
       "test_mse            25166999120397.688 +/- 76693698835336.06\n",
       "test_r2            -36256017250536.91 +/- 110486278267790.69\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fit_time            0.42221221923828123 +/- 0.05131153768803283\n",
       "score_time      0.005055356025695801 +/- 0.00036873005594932157\n",
       "test_mae               261206633.91949677 +/- 323014749.0456155\n",
       "test_mse       1.4406000138514101e+20 +/- 2.476592518474874e+20\n",
       "test_r2       -2.0753534699728844e+20 +/- 3.567822315359117e+20\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fit_time           0.39957308769226074 +/- 0.011466736511284614\n",
       "score_time       0.005256485939025879 +/- 0.0008528120118625178\n",
       "test_mae               705105055.0242938 +/- 1039987612.5173811\n",
       "test_mse       1.1464326202537182e+21 +/- 1.914064018026102e+21\n",
       "test_r2       -1.6515707994287629e+21 +/- 2.757433960409804e+21\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./lib/\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from lib.Methods import GeneralMethods\n",
    "from lib.edasSearch import EdasHyperparameterSearch\n",
    "from lib.Hiperparametros import HyperparameterSwitcher\n",
    "from lib.ImportacionModelos import getClassifierNames\n",
    "from lib.ImportacionModelos import getClassifierModels\n",
    "from lib.ImportacionModelos import getRegressorNames\n",
    "from lib.ImportacionModelos import getRegressorModels\n",
    "from lib.graphicGenerator import GraphicBuilder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.base import is_classifier\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics as scoreMetrics\n",
    "\n",
    "\n",
    "def accert(y_true, y_pred): \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return (cm.diagonal()/cm.sum(0)).mean()\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return scoreMetrics.mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "def mae(y_true, y_pred):\n",
    "    return scoreMetrics.mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def score_metrics(estimator, binary=False, params = {}):\n",
    "    score_ = {}\n",
    "    if (is_classifier(estimator)):\n",
    "        if (binary):\n",
    "            score_['average_precision'] = 'average_precision_weighted'\n",
    "            score_['precision'] = 'precision'\n",
    "            score_['recall'] = 'recall'\n",
    "            score_['balanced_accuracy'] = 'balanced_accuracy'\n",
    "            score_['roc_auc'] = 'roc_auc'\n",
    "        score_['nbaccuracy'] = make_scorer(accert)\n",
    "        score_['accuracy'] = 'accuracy'\n",
    "    else:\n",
    "        score_['mae'] = make_scorer(mae) # 'mean_absolute_error',\n",
    "        score_['mse'] = make_scorer(mse) # 'mean_squared_error',\n",
    "        # score['distance'] = make_scorer(distance2d)\n",
    "        score_['r2'] = 'r2'\n",
    "    return score_\n",
    "\n",
    "def cv_fold(estimator, n_splits=10, test_size=0.2, random_state=7):\n",
    "    if (is_classifier(estimator)):\n",
    "        return ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "    else:\n",
    "        return StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def clean_dataframe(df, input_columns, target_column, list_val):\n",
    "    x_train = df_train[input_columns]\n",
    "    y_train = df_train[target_column]\n",
    "    if (list_val[0]):\n",
    "        x_train = x_train.apply(lambda x: 100-x, axis=1)\n",
    "    if (list_val[1]):\n",
    "        x_train = preprocessing.scale(x_train)\n",
    "    if (list_val[2]):\n",
    "        x_train = pd.DataFrame(preprocessing.normalize(x_train), columns=wifi_columns) #preprocessing.normalize(x_train)\n",
    "    return x_train, y_train\n",
    "\n",
    "df_train = pd.read_csv(\"data/UJIndoorLoc_trainingData.csv\")\n",
    "df_test = pd.read_csv(\"data/UJIndoorLoc_validationData.csv\")\n",
    "\n",
    "wifi_columns = df_train.columns[:520]\n",
    "target_column = ['BUILDINGID', 'FLOOR', 'LATITUDE', 'LONGITUDE']\n",
    "building = 0\n",
    "floor = 1\n",
    "latitude = 2\n",
    "longitude = 3\n",
    "seed = 7\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "classifier_model = LogisticRegression()\n",
    "regression_model = KNeighborsRegressor()\n",
    "\n",
    "## train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
    "## test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "estimator = regression_model\n",
    "kf = cv_fold(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ['BUILDINGID', 'FLOOR', 'LATITUDE', 'LONGITUDE']\n",
    "building = 0\n",
    "floor = 1\n",
    "latitude = 2\n",
    "longitude = 3\n",
    "#output[building]\n",
    "\n",
    "wifi_columns = df_train.columns[:520]\n",
    "#x_train = df_train[wifi_columns]\n",
    "#y_train = df_train[output[building]]\n",
    "x_train, y_train = clean_dataframe(df_train, wifi_columns, target_column[latitude], [1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAP001</th>\n",
       "      <th>WAP002</th>\n",
       "      <th>WAP003</th>\n",
       "      <th>WAP004</th>\n",
       "      <th>WAP005</th>\n",
       "      <th>WAP006</th>\n",
       "      <th>WAP007</th>\n",
       "      <th>WAP008</th>\n",
       "      <th>WAP009</th>\n",
       "      <th>WAP010</th>\n",
       "      <th>...</th>\n",
       "      <th>WAP511</th>\n",
       "      <th>WAP512</th>\n",
       "      <th>WAP513</th>\n",
       "      <th>WAP514</th>\n",
       "      <th>WAP515</th>\n",
       "      <th>WAP516</th>\n",
       "      <th>WAP517</th>\n",
       "      <th>WAP518</th>\n",
       "      <th>WAP519</th>\n",
       "      <th>WAP520</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>-97</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 520 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
       "0     100     100     100     100     100     100     100     100     100   \n",
       "1     100     100     100     100     100     100     100     100     100   \n",
       "2     100     100     100     100     100     100     100     -97     100   \n",
       "3     100     100     100     100     100     100     100     100     100   \n",
       "4     100     100     100     100     100     100     100     100     100   \n",
       "\n",
       "   WAP010   ...    WAP511  WAP512  WAP513  WAP514  WAP515  WAP516  WAP517  \\\n",
       "0     100   ...       100     100     100     100     100     100     100   \n",
       "1     100   ...       100     100     100     100     100     100     100   \n",
       "2     100   ...       100     100     100     100     100     100     100   \n",
       "3     100   ...       100     100     100     100     100     100     100   \n",
       "4     100   ...       100     100     100     100     100     100     100   \n",
       "\n",
       "   WAP518  WAP519  WAP520  \n",
       "0     100     100     100  \n",
       "1     100     100     100  \n",
       "2     100     100     100  \n",
       "3     100     100     100  \n",
       "4     100     100     100  \n",
       "\n",
       "[5 rows x 520 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = df_train[wifi_columns]\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"data/UJIndoorLoc_trainingData.csv\")\n",
    "wifi_columns = df_train.columns[:520]\n",
    "x_train = df_train[wifi_columns]\n",
    "x_train_scaled = x_train.apply(lambda x: 100-x, axis=1)\n",
    "#x_train_scale = preprocessing.scale(x_train)\n",
    "x_min = 0\n",
    "x_max = 300\n",
    "x_train_normal = (x_train_scaled - x_min)/(x_max - x_min)\n",
    "x_train_normal['b'] = df_train[\"BUILDINGID\"]\n",
    "#pd.DataFrame(preprocessing.normalize(x_train), columns=wifi_columns) #preprocessing.normalize(x_train)\n",
    "#x_train_scale.head(10)\n",
    "#    if (list_val[1]):\n",
    "#       x_train = preprocessing.scale(x_train)\n",
    "#    if (list_val[2]):\n",
    "#        x_train = pd.DataFrame(preprocessing.normalize(x_train), columns=wifi_columns) #preprocessing.normalize(x_train)\n",
    "#    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_normal\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.base import is_classifier\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def msle(y_true, y_pred):\n",
    "    return mean_squared_log_error(y_true, y_pred)\n",
    "\n",
    "def cv_fold(estimator, n_splits=10, test_size=0.2, random_state=7):\n",
    "    if (is_classifier(estimator)):\n",
    "        return ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "    else:\n",
    "        return StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "\n",
    "def score_metrics(estimator, binary=False, params = {}):\n",
    "    score_ = {}\n",
    "    if (is_classifier(estimator)):\n",
    "        if (binary):\n",
    "            score_['average_precision'] = 'average_precision_weighted'\n",
    "            score_['precision'] = 'precision'\n",
    "            score_['recall'] = 'recall'\n",
    "            score_['balanced_accuracy'] = 'balanced_accuracy'\n",
    "            score_['roc_auc'] = 'roc_auc'\n",
    "        score_['nbaccuracy'] = make_scorer(accert)\n",
    "        score_['accuracy'] = 'accuracy'\n",
    "    else:\n",
    "        score_['mae'] = make_scorer(mae) # 'mean_absolute_error',\n",
    "        score_['mse'] = make_scorer(mse) # 'mean_squared_error',\n",
    "        #score_['msle'] = make_scorer(msle) # 'mean_squared_error',\n",
    "        # score['distance'] = make_scorer(distance2d)\n",
    "        score_['r2'] = 'r2'\n",
    "    return score_\n",
    "\n",
    "regression_model = KNeighborsRegressor()\n",
    "kf = cv_fold(regression_model)\n",
    "\n",
    "x_train = x_train_normal\n",
    "y_train = df_train['LONGITUDE']\n",
    "\n",
    "result = cross_validate(regression_model, x_train, y_train, scoring=score_metrics(regression_model), cv=kf, return_train_score=False)\n",
    "#np.max(np.max(x_train_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_time       0.689456\n",
       "score_time    56.293160\n",
       "test_mae       3.403231\n",
       "test_mse      44.652110\n",
       "test_r2        0.997071\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['LATITUDE']\n",
    "\n",
    "result = cross_validate(regression_model, x_train, y_train, scoring=score_metrics(regression_model), cv=kf, return_train_score=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_time       0.832683\n",
       "score_time    63.041220\n",
       "test_mae       2.805971\n",
       "test_mse      28.945055\n",
       "test_r2        0.993536\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.91223025, 0.80580807, 0.68192005, 0.85200214, 0.9109571 ,\n",
       "        0.9639461 , 0.73834801, 1.08377695, 0.76063395, 0.617208  ]),\n",
       " 'score_time': array([59.42688465, 68.73986006, 62.93775296, 64.1159339 , 74.16430902,\n",
       "        62.80884886, 62.56595421, 59.06078219, 57.24130487, 59.35056591]),\n",
       " 'test_mae': array([2.72822137, 2.8991892 , 2.77998527, 2.87009842, 2.78802629,\n",
       "        2.959681  , 2.71120111, 2.80026587, 2.76207241, 2.760969  ]),\n",
       " 'test_mse': array([26.52651818, 30.97082805, 30.27411075, 30.33410543, 26.91190117,\n",
       "        32.50071599, 26.82768424, 28.64234779, 30.29946511, 26.16287795]),\n",
       " 'test_r2': array([0.99407614, 0.99308495, 0.99324034, 0.9932267 , 0.9939912 ,\n",
       "        0.99274119, 0.9940083 , 0.99360205, 0.99323384, 0.99415684])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -12070340336739.46 (1789724221880.37) MSE\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(wifi_columns, input_dim=wifi_columns, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model\n",
    "\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=5, batch_size=5, verbose=0)\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, x_train_normal, y_train, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "WARNING:tensorflow:From /Users/ohuarcaya/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/ohuarcaya/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(254, activation=tf.nn.tanh, input_shape=[input_shape]),\n",
    "        layers.Dense(128, activation=tf.nn.softmax),\n",
    "        layers.Dense(128, activation=tf.nn.sigmoid),\n",
    "        layers.Dense(32, activation=tf.nn.relu),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "model = build_model(len(x_train.keys())) #520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 254)               132334    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32640     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 185,647\n",
      "Trainable params: 185,647\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set Mean Abs Error:  870.68695\n",
      "Testing set Mean Sqr Error:  762574.06\n"
     ]
    }
   ],
   "source": [
    "loss, mae, mse = model.evaluate(x_train, y_train-4864000, verbose=0)\n",
    "#max(y_train)-4864000\n",
    "print(\"Testing set Mean Abs Error:  \" + str(mae))\n",
    "\n",
    "print(\"Testing set Mean Sqr Error:  \" + str(mse))\n",
    "#23666960000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = clean_dataframe(df_test, wifi_columns, target_column[building], [1, 0, 1])\n",
    "test_predictions = model.predict(x_test).flatten()\n",
    "%matplotlib notebook\n",
    "plt.plot(y_test, test_predictions)\n",
    "plt.show()\n",
    "#plt.xlabel('True Values [MPG]')\n",
    "#plt.ylabel('Predictions [MPG]')\n",
    "#plt.axis('equal')\n",
    "#plt.axis('square')\n",
    "#plt.xlim([0,plt.xlim()[1]])\n",
    "#plt.ylim([0,plt.ylim()[1]])\n",
    "#_ = plt.plot([-100, 100], [-100, 100])\n",
    "#plt.plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    19937.000000\n",
       "mean         1.212820\n",
       "std          0.833139\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          1.000000\n",
       "75%          2.000000\n",
       "max          2.000000\n",
       "Name: BUILDINGID, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-e540781a8294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneural_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-e540781a8294>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(neural_network, X, Y, f_cost, lr, train)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneural_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# Calcular delta última capa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mdeltas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_cost\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_activacion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# Calcular delta respecto a capa previa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-e540781a8294>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(Yp, Yr)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m l2_cost = (lambda Yp, Yr: np.mean((Yp - Yr) ** 2),\n\u001b[0;32m---> 36\u001b[0;31m            lambda Yp, Yr: Yp - Yr)\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneural_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_na_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         return construct_result(left, result,\n\u001b[0;32m-> 1071\u001b[0;31m                                 index=left.index, name=res_name, dtype=None)\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m_construct_result\u001b[0;34m(left, result, index, name, dtype)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0menough\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mstill\u001b[0m \u001b[0mneed\u001b[0m \u001b[0mto\u001b[0m \u001b[0moverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m     \"\"\"\n\u001b[0;32m--> 980\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 data = _sanitize_array(data, index, dtype, copy,\n\u001b[0;32m--> 275\u001b[0;31m                                        raise_cast_failure=True)\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m   4163\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4165\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data must be 1-dimensional'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4166\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4167\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "import seaborn\n",
    "#import deap # !pip install deap\n",
    "\n",
    "f_sigmoide = (lambda x: 1/(1 + np.e ** (-x)),\n",
    "              lambda x: x * (1 - x))\n",
    "\n",
    "f_relu = lambda x: np.maximum(0, x)\n",
    "\n",
    "# CAPA DE RED\n",
    "class neural_layer():\n",
    "    def __init__(self, n_conexiones, n_neuronas, f_activacion):\n",
    "        self.f_activacion =  f_activacion\n",
    "        self.bias = np.random.rand(1, n_neuronas) * 2 - 1\n",
    "        self.w = np.random.rand(n_conexiones, n_neuronas) * 2 - 1\n",
    "\n",
    "# RED NEURONAL\n",
    "def create_nn(topology, f_activacion):\n",
    "    nn = []\n",
    "    for i, layer in enumerate(topology[:-1]):\n",
    "        nn.append(neural_layer(topology[i], topology[i+1], f_activacion))\n",
    "    return nn\n",
    "\n",
    "# ENTRENAMIENTO\n",
    "\n",
    "p = 520\n",
    "topology = [p, 260, 8, 16, 8, 4, 1]\n",
    "\n",
    "neural_network = create_nn(topology, f_sigmoide)\n",
    "\n",
    "l2_cost = (lambda Yp, Yr: np.mean((Yp - Yr) ** 2),\n",
    "           lambda Yp, Yr: Yp - Yr)\n",
    "\n",
    "def train(neural_network, X, Y, f_cost, lr=0.5, train=True):\n",
    "    out = [(None, X)]\n",
    "    # Forward pass\n",
    "    #z = X @neural_network[l].w + neural_network[1].bias\n",
    "    #a = neural_network[0].f_activacion(z)\n",
    "\n",
    "    for l, layer in enumerate(neural_network):\n",
    "        #z = out[-1][1] @ neural_network[l].w #+ neural_network[l].bias\n",
    "        #z = z + pd.concat([pd.DataFrame(neural_network[0].bias)]*z.shape[0], ignore_index=True) # Ignores the index\n",
    "        z = np.array(out[-1][1] @ neural_network[l].w) + neural_network[l].bias\n",
    "        a = neural_network[l].f_activacion[0](z)\n",
    "        out.append((z, a))\n",
    "        \n",
    "    #print(f_cost[0](out[-1][1], Y))\n",
    "\n",
    "    if train:\n",
    "        # Backward pass\n",
    "        deltas = []\n",
    "    \n",
    "    for l in reversed(range(0, len(neural_network))):\n",
    "        z = out[l+1][0]\n",
    "        a = out[l+1][1]\n",
    "\n",
    "        if l == len(neural_network)-1:\n",
    "            # Calcular delta última capa\n",
    "            deltas.insert(0, f_cost[1](a, Y) * neural_network[l].f_activacion[1](a))\n",
    "        else:\n",
    "            # Calcular delta respecto a capa previa\n",
    "            #print('1', deltas[0].shape, _w.shape)\n",
    "            deltas.insert(0, deltas[0] @ _w.T * neural_network[l].f_activacion[1](a))\n",
    "\n",
    "        _w = neural_network[l].w      \n",
    "\n",
    "        # Gradient descent\n",
    "        neural_network[l].bias = neural_network[l].bias - np.mean(deltas[0], axis=0, keepdims=True) * lr\n",
    "\n",
    "        #print('2', out[l][1].T.shape, deltas[0].shape)\n",
    "        neural_network[l].w = neural_network[l].w - out[l][1].T @ deltas[0] * lr\n",
    "\n",
    "    return out[-1][1]\n",
    "\n",
    "train(neural_network, x_train, y_train, l2_cost, 0.5)    \n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.80737522,  0.71085004, -0.30577569, ...,  0.0274235 ,\n",
       "        -1.0194942 , -0.09270976],\n",
       "       [ 1.30299393,  0.67712807,  0.22300127, ...,  0.10620161,\n",
       "        -0.84530922,  0.13510745],\n",
       "       [ 0.97525689,  0.90972751, -0.33691266, ..., -0.06938857,\n",
       "        -1.72811096, -0.17227256],\n",
       "       ...,\n",
       "       [ 0.55882098,  0.20607047, -0.44964348, ...,  0.49915801,\n",
       "        -1.11853866, -0.95742318],\n",
       "       [ 1.15273143,  0.39639255, -0.0632814 , ..., -0.38473683,\n",
       "        -0.75444506, -0.33986863],\n",
       "       [ 1.65125925,  1.05435892,  0.12561712, ..., -0.23774095,\n",
       "        -0.67544803, -0.37555323]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iteration = [[i%2, int(i/2)%2, int(i/4)] for i in range(8)]\n",
    "#np.array([[1,2,2],[1,2,3]]) + np.array([[1,2,4]])\n",
    "#test = [(None, x_train)]\n",
    "np.array(test[-1][1] @ neural_network[0].w) + neural_network[0].bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.285187</td>\n",
       "      <td>1.035191</td>\n",
       "      <td>1.682350</td>\n",
       "      <td>0.104763</td>\n",
       "      <td>-0.882211</td>\n",
       "      <td>-0.137103</td>\n",
       "      <td>-0.170671</td>\n",
       "      <td>1.225717</td>\n",
       "      <td>0.479215</td>\n",
       "      <td>-0.046635</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.151620</td>\n",
       "      <td>-1.350320</td>\n",
       "      <td>0.321935</td>\n",
       "      <td>-0.611737</td>\n",
       "      <td>0.164607</td>\n",
       "      <td>0.189246</td>\n",
       "      <td>-1.987858</td>\n",
       "      <td>0.204009</td>\n",
       "      <td>0.415615</td>\n",
       "      <td>-1.623930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.415295</td>\n",
       "      <td>-0.219920</td>\n",
       "      <td>1.657734</td>\n",
       "      <td>0.139547</td>\n",
       "      <td>-0.228371</td>\n",
       "      <td>0.232568</td>\n",
       "      <td>0.100882</td>\n",
       "      <td>0.351741</td>\n",
       "      <td>1.133607</td>\n",
       "      <td>-0.896950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039406</td>\n",
       "      <td>-0.948139</td>\n",
       "      <td>-0.052438</td>\n",
       "      <td>-0.566592</td>\n",
       "      <td>-0.314158</td>\n",
       "      <td>-0.007940</td>\n",
       "      <td>-1.963288</td>\n",
       "      <td>-0.075500</td>\n",
       "      <td>0.747609</td>\n",
       "      <td>-0.833278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.627834</td>\n",
       "      <td>0.471727</td>\n",
       "      <td>1.673070</td>\n",
       "      <td>-0.047936</td>\n",
       "      <td>-0.804701</td>\n",
       "      <td>-0.305792</td>\n",
       "      <td>-0.144297</td>\n",
       "      <td>1.070119</td>\n",
       "      <td>1.287865</td>\n",
       "      <td>-0.244582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.692564</td>\n",
       "      <td>-1.668460</td>\n",
       "      <td>0.260366</td>\n",
       "      <td>-0.797651</td>\n",
       "      <td>0.439347</td>\n",
       "      <td>0.235958</td>\n",
       "      <td>-1.521491</td>\n",
       "      <td>0.536750</td>\n",
       "      <td>0.221807</td>\n",
       "      <td>-1.815555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.458096</td>\n",
       "      <td>0.393705</td>\n",
       "      <td>1.368870</td>\n",
       "      <td>-0.532301</td>\n",
       "      <td>-0.976260</td>\n",
       "      <td>-0.014768</td>\n",
       "      <td>-0.083186</td>\n",
       "      <td>1.086714</td>\n",
       "      <td>1.146637</td>\n",
       "      <td>-0.849305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524199</td>\n",
       "      <td>-1.173215</td>\n",
       "      <td>0.617650</td>\n",
       "      <td>-1.194660</td>\n",
       "      <td>0.048380</td>\n",
       "      <td>0.426114</td>\n",
       "      <td>-1.373202</td>\n",
       "      <td>0.501936</td>\n",
       "      <td>0.940675</td>\n",
       "      <td>-1.607249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.406970</td>\n",
       "      <td>-0.212999</td>\n",
       "      <td>0.801990</td>\n",
       "      <td>-0.071332</td>\n",
       "      <td>-0.990373</td>\n",
       "      <td>-0.058411</td>\n",
       "      <td>0.054665</td>\n",
       "      <td>0.953116</td>\n",
       "      <td>0.228877</td>\n",
       "      <td>-0.635901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.891404</td>\n",
       "      <td>-0.702635</td>\n",
       "      <td>-0.048356</td>\n",
       "      <td>-0.515967</td>\n",
       "      <td>-0.334811</td>\n",
       "      <td>0.770778</td>\n",
       "      <td>-0.632434</td>\n",
       "      <td>-0.130604</td>\n",
       "      <td>0.713931</td>\n",
       "      <td>-0.288358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.879064</td>\n",
       "      <td>0.530055</td>\n",
       "      <td>1.249226</td>\n",
       "      <td>-0.306988</td>\n",
       "      <td>-0.174719</td>\n",
       "      <td>-0.108372</td>\n",
       "      <td>-0.099921</td>\n",
       "      <td>0.928785</td>\n",
       "      <td>1.009993</td>\n",
       "      <td>-0.044301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141253</td>\n",
       "      <td>-1.033423</td>\n",
       "      <td>0.320357</td>\n",
       "      <td>-0.758868</td>\n",
       "      <td>0.304653</td>\n",
       "      <td>0.542335</td>\n",
       "      <td>-1.863190</td>\n",
       "      <td>0.382143</td>\n",
       "      <td>0.461409</td>\n",
       "      <td>-0.911531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.606144</td>\n",
       "      <td>0.481296</td>\n",
       "      <td>1.404062</td>\n",
       "      <td>0.297868</td>\n",
       "      <td>-0.596131</td>\n",
       "      <td>-0.033229</td>\n",
       "      <td>-0.073326</td>\n",
       "      <td>0.941797</td>\n",
       "      <td>0.974575</td>\n",
       "      <td>-0.153837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496372</td>\n",
       "      <td>-1.701310</td>\n",
       "      <td>0.067612</td>\n",
       "      <td>-0.826262</td>\n",
       "      <td>0.641773</td>\n",
       "      <td>-0.028948</td>\n",
       "      <td>-1.980331</td>\n",
       "      <td>0.458563</td>\n",
       "      <td>0.286968</td>\n",
       "      <td>-1.450641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.758421</td>\n",
       "      <td>0.735004</td>\n",
       "      <td>1.905728</td>\n",
       "      <td>-0.301664</td>\n",
       "      <td>-0.361610</td>\n",
       "      <td>0.061511</td>\n",
       "      <td>-0.408666</td>\n",
       "      <td>0.946330</td>\n",
       "      <td>1.040725</td>\n",
       "      <td>-0.766435</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.417399</td>\n",
       "      <td>-1.496179</td>\n",
       "      <td>0.238060</td>\n",
       "      <td>-0.713422</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.191091</td>\n",
       "      <td>-1.666864</td>\n",
       "      <td>0.187643</td>\n",
       "      <td>0.942366</td>\n",
       "      <td>-1.543681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.226021</td>\n",
       "      <td>0.150854</td>\n",
       "      <td>1.476324</td>\n",
       "      <td>-0.401378</td>\n",
       "      <td>-1.471908</td>\n",
       "      <td>0.346949</td>\n",
       "      <td>-0.560114</td>\n",
       "      <td>1.153265</td>\n",
       "      <td>0.853556</td>\n",
       "      <td>-0.379513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057440</td>\n",
       "      <td>-0.719270</td>\n",
       "      <td>-0.867954</td>\n",
       "      <td>-0.089173</td>\n",
       "      <td>-0.595668</td>\n",
       "      <td>0.394974</td>\n",
       "      <td>-1.161239</td>\n",
       "      <td>-0.098599</td>\n",
       "      <td>0.909504</td>\n",
       "      <td>-0.683668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.494343</td>\n",
       "      <td>0.050526</td>\n",
       "      <td>0.829025</td>\n",
       "      <td>0.259778</td>\n",
       "      <td>-1.530470</td>\n",
       "      <td>0.309950</td>\n",
       "      <td>0.236543</td>\n",
       "      <td>0.625032</td>\n",
       "      <td>1.115580</td>\n",
       "      <td>-0.925240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.724463</td>\n",
       "      <td>-0.787428</td>\n",
       "      <td>-0.091150</td>\n",
       "      <td>0.406892</td>\n",
       "      <td>-0.781916</td>\n",
       "      <td>-0.022643</td>\n",
       "      <td>-1.317075</td>\n",
       "      <td>0.262037</td>\n",
       "      <td>-0.102802</td>\n",
       "      <td>-1.503233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.705172</td>\n",
       "      <td>0.154084</td>\n",
       "      <td>2.135681</td>\n",
       "      <td>-0.404793</td>\n",
       "      <td>-0.670155</td>\n",
       "      <td>0.238828</td>\n",
       "      <td>-0.116875</td>\n",
       "      <td>1.061091</td>\n",
       "      <td>2.077640</td>\n",
       "      <td>-0.113839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446646</td>\n",
       "      <td>-0.921451</td>\n",
       "      <td>-0.266021</td>\n",
       "      <td>-0.324262</td>\n",
       "      <td>-0.567364</td>\n",
       "      <td>0.726553</td>\n",
       "      <td>-1.432780</td>\n",
       "      <td>0.484867</td>\n",
       "      <td>0.625350</td>\n",
       "      <td>-1.499931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.298634</td>\n",
       "      <td>0.614322</td>\n",
       "      <td>1.764872</td>\n",
       "      <td>-0.182893</td>\n",
       "      <td>-0.485984</td>\n",
       "      <td>-0.490811</td>\n",
       "      <td>-0.278629</td>\n",
       "      <td>1.056378</td>\n",
       "      <td>0.643862</td>\n",
       "      <td>-0.494605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212744</td>\n",
       "      <td>-1.089869</td>\n",
       "      <td>0.304487</td>\n",
       "      <td>-0.164915</td>\n",
       "      <td>-0.479554</td>\n",
       "      <td>0.806182</td>\n",
       "      <td>-1.731338</td>\n",
       "      <td>0.129312</td>\n",
       "      <td>0.580873</td>\n",
       "      <td>-1.873682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.748262</td>\n",
       "      <td>0.159923</td>\n",
       "      <td>1.081423</td>\n",
       "      <td>0.203167</td>\n",
       "      <td>-1.023936</td>\n",
       "      <td>1.107684</td>\n",
       "      <td>0.017704</td>\n",
       "      <td>0.499456</td>\n",
       "      <td>0.134574</td>\n",
       "      <td>-0.628260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.822763</td>\n",
       "      <td>-0.299953</td>\n",
       "      <td>-0.376747</td>\n",
       "      <td>-0.579324</td>\n",
       "      <td>-0.578580</td>\n",
       "      <td>-0.185159</td>\n",
       "      <td>-0.512545</td>\n",
       "      <td>0.448680</td>\n",
       "      <td>0.254131</td>\n",
       "      <td>-0.699271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.611335</td>\n",
       "      <td>0.290104</td>\n",
       "      <td>1.193328</td>\n",
       "      <td>0.226539</td>\n",
       "      <td>-0.640593</td>\n",
       "      <td>0.892652</td>\n",
       "      <td>0.190626</td>\n",
       "      <td>0.681198</td>\n",
       "      <td>0.267802</td>\n",
       "      <td>-0.449859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664565</td>\n",
       "      <td>-0.396488</td>\n",
       "      <td>-0.096179</td>\n",
       "      <td>-0.314231</td>\n",
       "      <td>-0.359714</td>\n",
       "      <td>-0.085456</td>\n",
       "      <td>-0.882273</td>\n",
       "      <td>0.297162</td>\n",
       "      <td>0.293442</td>\n",
       "      <td>-0.685459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.324430</td>\n",
       "      <td>-0.147560</td>\n",
       "      <td>1.246451</td>\n",
       "      <td>0.125810</td>\n",
       "      <td>-0.828405</td>\n",
       "      <td>1.298855</td>\n",
       "      <td>-0.080084</td>\n",
       "      <td>0.907570</td>\n",
       "      <td>0.282221</td>\n",
       "      <td>-0.480228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392841</td>\n",
       "      <td>-0.615788</td>\n",
       "      <td>0.187657</td>\n",
       "      <td>-0.501334</td>\n",
       "      <td>-0.350578</td>\n",
       "      <td>-0.070487</td>\n",
       "      <td>-0.657295</td>\n",
       "      <td>-0.002836</td>\n",
       "      <td>0.408546</td>\n",
       "      <td>-0.865078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.116764</td>\n",
       "      <td>0.251204</td>\n",
       "      <td>1.590876</td>\n",
       "      <td>-0.019090</td>\n",
       "      <td>-1.101640</td>\n",
       "      <td>1.118139</td>\n",
       "      <td>-0.165261</td>\n",
       "      <td>0.612909</td>\n",
       "      <td>0.365554</td>\n",
       "      <td>-0.381981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430700</td>\n",
       "      <td>-0.218972</td>\n",
       "      <td>-0.255822</td>\n",
       "      <td>-0.404894</td>\n",
       "      <td>-0.321728</td>\n",
       "      <td>0.305401</td>\n",
       "      <td>-1.030368</td>\n",
       "      <td>0.597390</td>\n",
       "      <td>0.597561</td>\n",
       "      <td>-0.277033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.279634</td>\n",
       "      <td>0.197268</td>\n",
       "      <td>1.116551</td>\n",
       "      <td>0.057708</td>\n",
       "      <td>-1.350899</td>\n",
       "      <td>0.468304</td>\n",
       "      <td>-0.336921</td>\n",
       "      <td>0.640288</td>\n",
       "      <td>0.890486</td>\n",
       "      <td>-0.227668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.723747</td>\n",
       "      <td>-0.589558</td>\n",
       "      <td>-0.748660</td>\n",
       "      <td>-0.312159</td>\n",
       "      <td>-0.657484</td>\n",
       "      <td>0.029896</td>\n",
       "      <td>-1.169107</td>\n",
       "      <td>0.012457</td>\n",
       "      <td>0.836292</td>\n",
       "      <td>-0.268422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.092136</td>\n",
       "      <td>0.255844</td>\n",
       "      <td>1.622497</td>\n",
       "      <td>0.471705</td>\n",
       "      <td>-1.937269</td>\n",
       "      <td>1.571148</td>\n",
       "      <td>0.277214</td>\n",
       "      <td>1.470943</td>\n",
       "      <td>0.521168</td>\n",
       "      <td>-0.133855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134251</td>\n",
       "      <td>-0.882997</td>\n",
       "      <td>-0.349122</td>\n",
       "      <td>-0.360120</td>\n",
       "      <td>-1.010623</td>\n",
       "      <td>0.638444</td>\n",
       "      <td>-0.793697</td>\n",
       "      <td>0.026867</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>-0.751735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.344009</td>\n",
       "      <td>-1.119431</td>\n",
       "      <td>1.206854</td>\n",
       "      <td>0.114018</td>\n",
       "      <td>-1.561882</td>\n",
       "      <td>1.360267</td>\n",
       "      <td>0.475750</td>\n",
       "      <td>0.919540</td>\n",
       "      <td>0.387768</td>\n",
       "      <td>-0.678120</td>\n",
       "      <td>...</td>\n",
       "      <td>1.087092</td>\n",
       "      <td>-0.818499</td>\n",
       "      <td>0.033627</td>\n",
       "      <td>-0.518780</td>\n",
       "      <td>-0.371036</td>\n",
       "      <td>0.688531</td>\n",
       "      <td>-0.427074</td>\n",
       "      <td>0.124376</td>\n",
       "      <td>0.458212</td>\n",
       "      <td>-0.040441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.346075</td>\n",
       "      <td>0.736056</td>\n",
       "      <td>1.842471</td>\n",
       "      <td>0.560988</td>\n",
       "      <td>-1.089234</td>\n",
       "      <td>0.353329</td>\n",
       "      <td>-0.178329</td>\n",
       "      <td>0.086533</td>\n",
       "      <td>0.274926</td>\n",
       "      <td>-0.168760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.243486</td>\n",
       "      <td>-0.841205</td>\n",
       "      <td>0.203325</td>\n",
       "      <td>-0.195250</td>\n",
       "      <td>-0.451563</td>\n",
       "      <td>-0.469346</td>\n",
       "      <td>-1.979240</td>\n",
       "      <td>-0.025462</td>\n",
       "      <td>0.655013</td>\n",
       "      <td>-0.851254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.056211</td>\n",
       "      <td>0.043081</td>\n",
       "      <td>1.640319</td>\n",
       "      <td>0.356446</td>\n",
       "      <td>-0.652944</td>\n",
       "      <td>0.714529</td>\n",
       "      <td>0.318359</td>\n",
       "      <td>0.902505</td>\n",
       "      <td>0.410563</td>\n",
       "      <td>-0.411312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.876566</td>\n",
       "      <td>-0.244997</td>\n",
       "      <td>-0.213275</td>\n",
       "      <td>-0.418808</td>\n",
       "      <td>-1.141672</td>\n",
       "      <td>-0.295635</td>\n",
       "      <td>-1.696517</td>\n",
       "      <td>0.097755</td>\n",
       "      <td>0.169520</td>\n",
       "      <td>-0.631464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.422841</td>\n",
       "      <td>-0.373147</td>\n",
       "      <td>0.864455</td>\n",
       "      <td>0.435885</td>\n",
       "      <td>-2.057341</td>\n",
       "      <td>1.404463</td>\n",
       "      <td>0.661329</td>\n",
       "      <td>1.482676</td>\n",
       "      <td>0.012479</td>\n",
       "      <td>-0.994905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391724</td>\n",
       "      <td>-0.641041</td>\n",
       "      <td>-0.257436</td>\n",
       "      <td>-0.352003</td>\n",
       "      <td>-0.802625</td>\n",
       "      <td>1.069941</td>\n",
       "      <td>-0.304242</td>\n",
       "      <td>-0.399835</td>\n",
       "      <td>0.844856</td>\n",
       "      <td>0.038447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.244289</td>\n",
       "      <td>-0.666319</td>\n",
       "      <td>1.469011</td>\n",
       "      <td>0.285309</td>\n",
       "      <td>-0.981507</td>\n",
       "      <td>1.232740</td>\n",
       "      <td>0.299815</td>\n",
       "      <td>0.782959</td>\n",
       "      <td>0.123607</td>\n",
       "      <td>-0.977749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525912</td>\n",
       "      <td>-0.981852</td>\n",
       "      <td>0.470226</td>\n",
       "      <td>-0.519586</td>\n",
       "      <td>-0.213811</td>\n",
       "      <td>0.358933</td>\n",
       "      <td>-0.439318</td>\n",
       "      <td>-0.066122</td>\n",
       "      <td>-0.166237</td>\n",
       "      <td>-0.120940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.153335</td>\n",
       "      <td>-0.327218</td>\n",
       "      <td>1.588007</td>\n",
       "      <td>0.176319</td>\n",
       "      <td>-1.989055</td>\n",
       "      <td>0.961840</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>1.145125</td>\n",
       "      <td>0.691538</td>\n",
       "      <td>-0.680728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424098</td>\n",
       "      <td>-0.162036</td>\n",
       "      <td>0.109003</td>\n",
       "      <td>-0.951403</td>\n",
       "      <td>-0.986287</td>\n",
       "      <td>0.744978</td>\n",
       "      <td>-0.361658</td>\n",
       "      <td>0.239314</td>\n",
       "      <td>0.502896</td>\n",
       "      <td>-0.985339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.603135</td>\n",
       "      <td>0.070254</td>\n",
       "      <td>1.621128</td>\n",
       "      <td>0.840278</td>\n",
       "      <td>-0.478582</td>\n",
       "      <td>-0.015553</td>\n",
       "      <td>0.541238</td>\n",
       "      <td>1.157969</td>\n",
       "      <td>0.192505</td>\n",
       "      <td>-0.064892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961037</td>\n",
       "      <td>0.010108</td>\n",
       "      <td>0.375949</td>\n",
       "      <td>-0.259983</td>\n",
       "      <td>-1.506536</td>\n",
       "      <td>0.517542</td>\n",
       "      <td>-1.463983</td>\n",
       "      <td>-0.281955</td>\n",
       "      <td>-0.590905</td>\n",
       "      <td>-1.198447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.242318</td>\n",
       "      <td>-0.696743</td>\n",
       "      <td>1.178905</td>\n",
       "      <td>0.282141</td>\n",
       "      <td>-1.531175</td>\n",
       "      <td>1.029963</td>\n",
       "      <td>0.463092</td>\n",
       "      <td>1.274598</td>\n",
       "      <td>-0.090282</td>\n",
       "      <td>-1.123560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925937</td>\n",
       "      <td>-0.553136</td>\n",
       "      <td>0.473705</td>\n",
       "      <td>-0.289715</td>\n",
       "      <td>-0.716616</td>\n",
       "      <td>0.612532</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.029923</td>\n",
       "      <td>-0.172505</td>\n",
       "      <td>-0.109875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.681934</td>\n",
       "      <td>-0.517908</td>\n",
       "      <td>0.801893</td>\n",
       "      <td>0.651121</td>\n",
       "      <td>-0.818573</td>\n",
       "      <td>1.394127</td>\n",
       "      <td>0.330933</td>\n",
       "      <td>1.229115</td>\n",
       "      <td>-0.166330</td>\n",
       "      <td>-0.831579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.921571</td>\n",
       "      <td>-0.862533</td>\n",
       "      <td>0.719016</td>\n",
       "      <td>0.269031</td>\n",
       "      <td>-0.827607</td>\n",
       "      <td>0.131635</td>\n",
       "      <td>-0.954797</td>\n",
       "      <td>0.333096</td>\n",
       "      <td>-0.148470</td>\n",
       "      <td>-0.621245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.702217</td>\n",
       "      <td>-0.970979</td>\n",
       "      <td>1.065473</td>\n",
       "      <td>0.568281</td>\n",
       "      <td>-1.581719</td>\n",
       "      <td>1.438031</td>\n",
       "      <td>1.089776</td>\n",
       "      <td>0.715492</td>\n",
       "      <td>0.571947</td>\n",
       "      <td>-0.630926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781380</td>\n",
       "      <td>-0.977994</td>\n",
       "      <td>0.460930</td>\n",
       "      <td>-0.543460</td>\n",
       "      <td>-0.249600</td>\n",
       "      <td>0.293791</td>\n",
       "      <td>-0.532622</td>\n",
       "      <td>0.555038</td>\n",
       "      <td>0.267074</td>\n",
       "      <td>-0.661679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.407817</td>\n",
       "      <td>-0.562328</td>\n",
       "      <td>1.426271</td>\n",
       "      <td>0.317435</td>\n",
       "      <td>-2.243041</td>\n",
       "      <td>1.521384</td>\n",
       "      <td>0.540068</td>\n",
       "      <td>0.874356</td>\n",
       "      <td>-0.457330</td>\n",
       "      <td>-0.840793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441035</td>\n",
       "      <td>-0.292092</td>\n",
       "      <td>0.489717</td>\n",
       "      <td>-0.509199</td>\n",
       "      <td>-0.607702</td>\n",
       "      <td>0.980750</td>\n",
       "      <td>0.010450</td>\n",
       "      <td>-0.286908</td>\n",
       "      <td>0.355200</td>\n",
       "      <td>-0.144650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.134201</td>\n",
       "      <td>-0.700121</td>\n",
       "      <td>1.145826</td>\n",
       "      <td>0.345384</td>\n",
       "      <td>-1.634236</td>\n",
       "      <td>0.964431</td>\n",
       "      <td>-0.114421</td>\n",
       "      <td>1.492166</td>\n",
       "      <td>0.572583</td>\n",
       "      <td>-0.868971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.890958</td>\n",
       "      <td>-0.399293</td>\n",
       "      <td>0.544484</td>\n",
       "      <td>0.113059</td>\n",
       "      <td>-0.739818</td>\n",
       "      <td>0.757374</td>\n",
       "      <td>-0.015978</td>\n",
       "      <td>0.160122</td>\n",
       "      <td>-0.081263</td>\n",
       "      <td>-0.466850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19907</th>\n",
       "      <td>0.174146</td>\n",
       "      <td>0.824128</td>\n",
       "      <td>0.992666</td>\n",
       "      <td>-0.030849</td>\n",
       "      <td>-1.412738</td>\n",
       "      <td>0.595410</td>\n",
       "      <td>-0.309645</td>\n",
       "      <td>0.801669</td>\n",
       "      <td>0.920571</td>\n",
       "      <td>-0.583945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126044</td>\n",
       "      <td>-1.298162</td>\n",
       "      <td>0.138345</td>\n",
       "      <td>0.057977</td>\n",
       "      <td>-0.735195</td>\n",
       "      <td>-0.042736</td>\n",
       "      <td>-1.629146</td>\n",
       "      <td>0.596539</td>\n",
       "      <td>0.149328</td>\n",
       "      <td>-1.258806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19908</th>\n",
       "      <td>0.695981</td>\n",
       "      <td>0.826697</td>\n",
       "      <td>1.224926</td>\n",
       "      <td>-0.109624</td>\n",
       "      <td>-0.616410</td>\n",
       "      <td>-0.048770</td>\n",
       "      <td>0.624648</td>\n",
       "      <td>0.829235</td>\n",
       "      <td>0.343069</td>\n",
       "      <td>-0.817465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112368</td>\n",
       "      <td>-0.934944</td>\n",
       "      <td>0.871879</td>\n",
       "      <td>0.183696</td>\n",
       "      <td>-0.068529</td>\n",
       "      <td>0.243585</td>\n",
       "      <td>-1.985366</td>\n",
       "      <td>0.284895</td>\n",
       "      <td>-0.025500</td>\n",
       "      <td>-1.064124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19909</th>\n",
       "      <td>0.568925</td>\n",
       "      <td>0.557675</td>\n",
       "      <td>0.242264</td>\n",
       "      <td>0.564490</td>\n",
       "      <td>-0.899195</td>\n",
       "      <td>0.752759</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>1.137420</td>\n",
       "      <td>0.184258</td>\n",
       "      <td>-0.779983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382551</td>\n",
       "      <td>-1.071041</td>\n",
       "      <td>0.661890</td>\n",
       "      <td>-0.046969</td>\n",
       "      <td>-0.902392</td>\n",
       "      <td>0.394801</td>\n",
       "      <td>-1.293081</td>\n",
       "      <td>-0.147650</td>\n",
       "      <td>0.531089</td>\n",
       "      <td>-1.026419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19910</th>\n",
       "      <td>-0.325505</td>\n",
       "      <td>0.006913</td>\n",
       "      <td>-0.167568</td>\n",
       "      <td>-0.043454</td>\n",
       "      <td>-1.122498</td>\n",
       "      <td>0.032773</td>\n",
       "      <td>-0.138201</td>\n",
       "      <td>1.584826</td>\n",
       "      <td>-0.175997</td>\n",
       "      <td>0.755036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.930859</td>\n",
       "      <td>-0.170287</td>\n",
       "      <td>0.146898</td>\n",
       "      <td>-0.821167</td>\n",
       "      <td>-0.649653</td>\n",
       "      <td>-0.028856</td>\n",
       "      <td>0.617293</td>\n",
       "      <td>-0.157290</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>-0.174713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19911</th>\n",
       "      <td>0.483715</td>\n",
       "      <td>-0.621755</td>\n",
       "      <td>0.487636</td>\n",
       "      <td>-0.716280</td>\n",
       "      <td>-1.833009</td>\n",
       "      <td>0.406727</td>\n",
       "      <td>0.329624</td>\n",
       "      <td>0.610548</td>\n",
       "      <td>-0.020123</td>\n",
       "      <td>1.221650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568632</td>\n",
       "      <td>-0.791973</td>\n",
       "      <td>0.585378</td>\n",
       "      <td>-0.761671</td>\n",
       "      <td>-0.782381</td>\n",
       "      <td>0.362813</td>\n",
       "      <td>0.952196</td>\n",
       "      <td>0.722977</td>\n",
       "      <td>0.453143</td>\n",
       "      <td>-1.116172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19912</th>\n",
       "      <td>-0.876192</td>\n",
       "      <td>-0.155553</td>\n",
       "      <td>0.035603</td>\n",
       "      <td>-0.788664</td>\n",
       "      <td>-0.875220</td>\n",
       "      <td>-0.180997</td>\n",
       "      <td>-0.361549</td>\n",
       "      <td>1.416749</td>\n",
       "      <td>-0.281474</td>\n",
       "      <td>0.311666</td>\n",
       "      <td>...</td>\n",
       "      <td>1.126562</td>\n",
       "      <td>0.622560</td>\n",
       "      <td>-0.030787</td>\n",
       "      <td>-1.036571</td>\n",
       "      <td>-1.407478</td>\n",
       "      <td>1.109743</td>\n",
       "      <td>0.482682</td>\n",
       "      <td>-0.136174</td>\n",
       "      <td>0.779005</td>\n",
       "      <td>-0.038232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19913</th>\n",
       "      <td>-0.613963</td>\n",
       "      <td>0.056403</td>\n",
       "      <td>0.227106</td>\n",
       "      <td>0.154741</td>\n",
       "      <td>-0.837306</td>\n",
       "      <td>-0.238049</td>\n",
       "      <td>0.920116</td>\n",
       "      <td>0.827699</td>\n",
       "      <td>-0.235098</td>\n",
       "      <td>0.352090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919164</td>\n",
       "      <td>-0.277150</td>\n",
       "      <td>-0.362255</td>\n",
       "      <td>-0.542223</td>\n",
       "      <td>-1.032610</td>\n",
       "      <td>0.406903</td>\n",
       "      <td>0.728829</td>\n",
       "      <td>0.280514</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>-0.415873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19914</th>\n",
       "      <td>-0.301567</td>\n",
       "      <td>-0.295941</td>\n",
       "      <td>0.263274</td>\n",
       "      <td>-0.240122</td>\n",
       "      <td>-1.063000</td>\n",
       "      <td>-0.147889</td>\n",
       "      <td>0.263024</td>\n",
       "      <td>0.479348</td>\n",
       "      <td>-0.169673</td>\n",
       "      <td>-0.087236</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011245</td>\n",
       "      <td>-0.137989</td>\n",
       "      <td>-1.150331</td>\n",
       "      <td>-0.324849</td>\n",
       "      <td>-2.330701</td>\n",
       "      <td>1.025317</td>\n",
       "      <td>1.026803</td>\n",
       "      <td>-0.111122</td>\n",
       "      <td>0.836173</td>\n",
       "      <td>-0.356987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19915</th>\n",
       "      <td>-0.546964</td>\n",
       "      <td>0.037918</td>\n",
       "      <td>0.138908</td>\n",
       "      <td>-0.085343</td>\n",
       "      <td>-0.792171</td>\n",
       "      <td>0.305971</td>\n",
       "      <td>0.298391</td>\n",
       "      <td>1.695459</td>\n",
       "      <td>0.352275</td>\n",
       "      <td>0.112681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694319</td>\n",
       "      <td>0.175414</td>\n",
       "      <td>-0.287363</td>\n",
       "      <td>-0.821994</td>\n",
       "      <td>-1.523808</td>\n",
       "      <td>0.748204</td>\n",
       "      <td>0.318738</td>\n",
       "      <td>-0.763715</td>\n",
       "      <td>0.959687</td>\n",
       "      <td>-1.096544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19916</th>\n",
       "      <td>-0.525858</td>\n",
       "      <td>0.249089</td>\n",
       "      <td>0.606795</td>\n",
       "      <td>0.548324</td>\n",
       "      <td>-1.893947</td>\n",
       "      <td>0.271687</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>1.294056</td>\n",
       "      <td>-0.041803</td>\n",
       "      <td>-0.360426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783156</td>\n",
       "      <td>0.490598</td>\n",
       "      <td>-0.494018</td>\n",
       "      <td>-0.150094</td>\n",
       "      <td>-2.434743</td>\n",
       "      <td>1.183848</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.277010</td>\n",
       "      <td>1.216865</td>\n",
       "      <td>-0.994504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19917</th>\n",
       "      <td>-0.340857</td>\n",
       "      <td>-0.400098</td>\n",
       "      <td>1.023823</td>\n",
       "      <td>0.511328</td>\n",
       "      <td>-1.514218</td>\n",
       "      <td>0.123152</td>\n",
       "      <td>-0.192541</td>\n",
       "      <td>1.516669</td>\n",
       "      <td>0.099132</td>\n",
       "      <td>-0.112112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.876835</td>\n",
       "      <td>0.363048</td>\n",
       "      <td>0.099218</td>\n",
       "      <td>-0.074914</td>\n",
       "      <td>-1.664705</td>\n",
       "      <td>0.439400</td>\n",
       "      <td>-0.102686</td>\n",
       "      <td>0.215025</td>\n",
       "      <td>0.753520</td>\n",
       "      <td>-2.192087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19918</th>\n",
       "      <td>-0.075984</td>\n",
       "      <td>-0.481886</td>\n",
       "      <td>0.251686</td>\n",
       "      <td>0.238351</td>\n",
       "      <td>-1.393104</td>\n",
       "      <td>-0.089930</td>\n",
       "      <td>-0.015873</td>\n",
       "      <td>1.253157</td>\n",
       "      <td>0.079276</td>\n",
       "      <td>0.033953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.630640</td>\n",
       "      <td>0.381187</td>\n",
       "      <td>0.061986</td>\n",
       "      <td>-0.648451</td>\n",
       "      <td>-0.872256</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>-0.107316</td>\n",
       "      <td>-0.297673</td>\n",
       "      <td>0.733173</td>\n",
       "      <td>-1.506285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19919</th>\n",
       "      <td>-0.163019</td>\n",
       "      <td>-0.393016</td>\n",
       "      <td>0.874340</td>\n",
       "      <td>-0.606862</td>\n",
       "      <td>-1.515917</td>\n",
       "      <td>-0.302979</td>\n",
       "      <td>0.702096</td>\n",
       "      <td>1.607521</td>\n",
       "      <td>-0.395949</td>\n",
       "      <td>0.501853</td>\n",
       "      <td>...</td>\n",
       "      <td>1.046992</td>\n",
       "      <td>0.388692</td>\n",
       "      <td>0.304665</td>\n",
       "      <td>-0.124389</td>\n",
       "      <td>-1.269499</td>\n",
       "      <td>0.438265</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>-0.260324</td>\n",
       "      <td>-0.113101</td>\n",
       "      <td>-1.719341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19920</th>\n",
       "      <td>-0.347073</td>\n",
       "      <td>0.210814</td>\n",
       "      <td>0.367967</td>\n",
       "      <td>0.065793</td>\n",
       "      <td>-1.145852</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>-0.321794</td>\n",
       "      <td>1.519922</td>\n",
       "      <td>0.145669</td>\n",
       "      <td>0.321942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227727</td>\n",
       "      <td>0.202683</td>\n",
       "      <td>-0.039522</td>\n",
       "      <td>-0.809958</td>\n",
       "      <td>-1.490975</td>\n",
       "      <td>0.453533</td>\n",
       "      <td>0.006769</td>\n",
       "      <td>-0.660903</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>-0.909558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19921</th>\n",
       "      <td>0.011231</td>\n",
       "      <td>0.086537</td>\n",
       "      <td>0.680752</td>\n",
       "      <td>-0.944560</td>\n",
       "      <td>-1.150014</td>\n",
       "      <td>0.373362</td>\n",
       "      <td>-0.151931</td>\n",
       "      <td>1.472334</td>\n",
       "      <td>0.040101</td>\n",
       "      <td>-0.554558</td>\n",
       "      <td>...</td>\n",
       "      <td>1.099976</td>\n",
       "      <td>-0.168161</td>\n",
       "      <td>-1.033424</td>\n",
       "      <td>-1.207961</td>\n",
       "      <td>-1.421215</td>\n",
       "      <td>1.299597</td>\n",
       "      <td>0.753097</td>\n",
       "      <td>-1.069383</td>\n",
       "      <td>0.935988</td>\n",
       "      <td>-0.570107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19922</th>\n",
       "      <td>-0.708079</td>\n",
       "      <td>0.214840</td>\n",
       "      <td>0.248001</td>\n",
       "      <td>0.887576</td>\n",
       "      <td>-1.118023</td>\n",
       "      <td>0.191037</td>\n",
       "      <td>0.698078</td>\n",
       "      <td>1.639054</td>\n",
       "      <td>-0.326507</td>\n",
       "      <td>-0.240289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327596</td>\n",
       "      <td>0.357065</td>\n",
       "      <td>-0.069434</td>\n",
       "      <td>-1.036808</td>\n",
       "      <td>-1.304366</td>\n",
       "      <td>0.303576</td>\n",
       "      <td>-0.248055</td>\n",
       "      <td>-0.367064</td>\n",
       "      <td>0.655427</td>\n",
       "      <td>-1.266412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19923</th>\n",
       "      <td>-0.057824</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.425260</td>\n",
       "      <td>0.267986</td>\n",
       "      <td>-0.960930</td>\n",
       "      <td>-0.342984</td>\n",
       "      <td>-0.158363</td>\n",
       "      <td>1.308088</td>\n",
       "      <td>-0.377870</td>\n",
       "      <td>0.227946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.437802</td>\n",
       "      <td>0.456166</td>\n",
       "      <td>-0.318535</td>\n",
       "      <td>-0.145937</td>\n",
       "      <td>-1.667207</td>\n",
       "      <td>0.407271</td>\n",
       "      <td>-0.281128</td>\n",
       "      <td>-0.007691</td>\n",
       "      <td>0.524799</td>\n",
       "      <td>-1.192680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19924</th>\n",
       "      <td>-1.128611</td>\n",
       "      <td>0.465557</td>\n",
       "      <td>0.194950</td>\n",
       "      <td>0.110801</td>\n",
       "      <td>-1.225296</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.134170</td>\n",
       "      <td>1.903555</td>\n",
       "      <td>-0.244991</td>\n",
       "      <td>0.677518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421385</td>\n",
       "      <td>0.501519</td>\n",
       "      <td>-0.207746</td>\n",
       "      <td>-0.252077</td>\n",
       "      <td>-0.914632</td>\n",
       "      <td>0.106953</td>\n",
       "      <td>0.269106</td>\n",
       "      <td>-0.974446</td>\n",
       "      <td>0.800509</td>\n",
       "      <td>-1.633075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19925</th>\n",
       "      <td>-0.631955</td>\n",
       "      <td>0.144171</td>\n",
       "      <td>0.493337</td>\n",
       "      <td>-0.444009</td>\n",
       "      <td>-0.669438</td>\n",
       "      <td>0.163358</td>\n",
       "      <td>0.202987</td>\n",
       "      <td>1.073492</td>\n",
       "      <td>0.599670</td>\n",
       "      <td>-0.343284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444883</td>\n",
       "      <td>0.666261</td>\n",
       "      <td>0.178824</td>\n",
       "      <td>-0.770191</td>\n",
       "      <td>-0.144058</td>\n",
       "      <td>0.829824</td>\n",
       "      <td>-0.099973</td>\n",
       "      <td>0.131971</td>\n",
       "      <td>0.516922</td>\n",
       "      <td>-1.129611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19926</th>\n",
       "      <td>-0.350087</td>\n",
       "      <td>0.504502</td>\n",
       "      <td>0.277488</td>\n",
       "      <td>-0.594621</td>\n",
       "      <td>-0.939992</td>\n",
       "      <td>0.326250</td>\n",
       "      <td>-0.137488</td>\n",
       "      <td>1.069369</td>\n",
       "      <td>0.392929</td>\n",
       "      <td>0.631852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.656610</td>\n",
       "      <td>0.117592</td>\n",
       "      <td>-0.298384</td>\n",
       "      <td>-1.435194</td>\n",
       "      <td>-0.986214</td>\n",
       "      <td>0.699533</td>\n",
       "      <td>0.288776</td>\n",
       "      <td>-0.193862</td>\n",
       "      <td>0.339595</td>\n",
       "      <td>-0.696602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19927</th>\n",
       "      <td>-0.548712</td>\n",
       "      <td>0.045855</td>\n",
       "      <td>0.479962</td>\n",
       "      <td>-0.354462</td>\n",
       "      <td>-0.812022</td>\n",
       "      <td>-0.062447</td>\n",
       "      <td>0.165850</td>\n",
       "      <td>2.033219</td>\n",
       "      <td>-0.262042</td>\n",
       "      <td>0.389151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945431</td>\n",
       "      <td>0.290358</td>\n",
       "      <td>0.119895</td>\n",
       "      <td>0.455461</td>\n",
       "      <td>-1.029226</td>\n",
       "      <td>0.709587</td>\n",
       "      <td>0.512314</td>\n",
       "      <td>-0.077732</td>\n",
       "      <td>0.857862</td>\n",
       "      <td>-1.561407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19928</th>\n",
       "      <td>-0.103767</td>\n",
       "      <td>-0.268599</td>\n",
       "      <td>0.354089</td>\n",
       "      <td>0.103874</td>\n",
       "      <td>-1.339020</td>\n",
       "      <td>-0.405641</td>\n",
       "      <td>-0.020696</td>\n",
       "      <td>2.081672</td>\n",
       "      <td>-0.295670</td>\n",
       "      <td>0.621130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860067</td>\n",
       "      <td>0.363640</td>\n",
       "      <td>-0.312848</td>\n",
       "      <td>-0.393726</td>\n",
       "      <td>-1.915523</td>\n",
       "      <td>0.539097</td>\n",
       "      <td>0.922638</td>\n",
       "      <td>0.305432</td>\n",
       "      <td>0.361016</td>\n",
       "      <td>-0.756502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19929</th>\n",
       "      <td>0.432389</td>\n",
       "      <td>-0.993138</td>\n",
       "      <td>1.169832</td>\n",
       "      <td>0.448713</td>\n",
       "      <td>-1.649643</td>\n",
       "      <td>1.357875</td>\n",
       "      <td>0.610546</td>\n",
       "      <td>0.970406</td>\n",
       "      <td>0.325847</td>\n",
       "      <td>-0.786682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.845192</td>\n",
       "      <td>-0.495958</td>\n",
       "      <td>0.160524</td>\n",
       "      <td>-0.721510</td>\n",
       "      <td>-0.444435</td>\n",
       "      <td>0.594968</td>\n",
       "      <td>-0.398151</td>\n",
       "      <td>0.271303</td>\n",
       "      <td>0.219992</td>\n",
       "      <td>-0.327732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19930</th>\n",
       "      <td>-0.015565</td>\n",
       "      <td>-0.463616</td>\n",
       "      <td>0.786974</td>\n",
       "      <td>0.794398</td>\n",
       "      <td>-0.818871</td>\n",
       "      <td>1.302955</td>\n",
       "      <td>-0.004605</td>\n",
       "      <td>0.551007</td>\n",
       "      <td>0.340697</td>\n",
       "      <td>-1.377090</td>\n",
       "      <td>...</td>\n",
       "      <td>1.077384</td>\n",
       "      <td>-0.487117</td>\n",
       "      <td>0.248811</td>\n",
       "      <td>0.172261</td>\n",
       "      <td>-0.881916</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>-0.744332</td>\n",
       "      <td>0.762182</td>\n",
       "      <td>0.744539</td>\n",
       "      <td>-0.961440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19931</th>\n",
       "      <td>-0.364753</td>\n",
       "      <td>0.179435</td>\n",
       "      <td>0.323016</td>\n",
       "      <td>-1.015851</td>\n",
       "      <td>-0.369367</td>\n",
       "      <td>-0.031561</td>\n",
       "      <td>-0.254441</td>\n",
       "      <td>1.076610</td>\n",
       "      <td>1.221405</td>\n",
       "      <td>-0.745442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609532</td>\n",
       "      <td>-0.957707</td>\n",
       "      <td>-0.318597</td>\n",
       "      <td>0.035867</td>\n",
       "      <td>-0.517285</td>\n",
       "      <td>1.049349</td>\n",
       "      <td>-0.238038</td>\n",
       "      <td>-0.414994</td>\n",
       "      <td>0.437721</td>\n",
       "      <td>0.104299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19932</th>\n",
       "      <td>0.015063</td>\n",
       "      <td>-0.592425</td>\n",
       "      <td>1.442457</td>\n",
       "      <td>0.201672</td>\n",
       "      <td>-1.745115</td>\n",
       "      <td>1.081401</td>\n",
       "      <td>0.665293</td>\n",
       "      <td>1.004689</td>\n",
       "      <td>-0.170649</td>\n",
       "      <td>-0.982375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909010</td>\n",
       "      <td>-0.504377</td>\n",
       "      <td>0.332382</td>\n",
       "      <td>-0.353062</td>\n",
       "      <td>-0.590654</td>\n",
       "      <td>0.368110</td>\n",
       "      <td>-0.337343</td>\n",
       "      <td>0.311092</td>\n",
       "      <td>0.412479</td>\n",
       "      <td>-0.193659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19933</th>\n",
       "      <td>0.128494</td>\n",
       "      <td>0.520338</td>\n",
       "      <td>1.358082</td>\n",
       "      <td>-0.635189</td>\n",
       "      <td>-0.499592</td>\n",
       "      <td>-0.432534</td>\n",
       "      <td>0.027534</td>\n",
       "      <td>1.110284</td>\n",
       "      <td>0.104205</td>\n",
       "      <td>0.208385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618037</td>\n",
       "      <td>0.519769</td>\n",
       "      <td>1.411446</td>\n",
       "      <td>-0.344667</td>\n",
       "      <td>-0.953240</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>-0.527777</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>-0.045325</td>\n",
       "      <td>-0.294871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19934</th>\n",
       "      <td>-0.135650</td>\n",
       "      <td>-0.933318</td>\n",
       "      <td>1.163087</td>\n",
       "      <td>-0.009839</td>\n",
       "      <td>-1.472165</td>\n",
       "      <td>0.873699</td>\n",
       "      <td>0.784119</td>\n",
       "      <td>0.950420</td>\n",
       "      <td>-0.050132</td>\n",
       "      <td>-1.339624</td>\n",
       "      <td>...</td>\n",
       "      <td>1.219578</td>\n",
       "      <td>-0.185845</td>\n",
       "      <td>0.121810</td>\n",
       "      <td>-0.085776</td>\n",
       "      <td>-0.999106</td>\n",
       "      <td>-0.037240</td>\n",
       "      <td>-0.692400</td>\n",
       "      <td>0.448869</td>\n",
       "      <td>0.400963</td>\n",
       "      <td>-0.139010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19935</th>\n",
       "      <td>0.571342</td>\n",
       "      <td>0.572098</td>\n",
       "      <td>1.033618</td>\n",
       "      <td>0.453356</td>\n",
       "      <td>-0.800633</td>\n",
       "      <td>1.801874</td>\n",
       "      <td>0.459235</td>\n",
       "      <td>0.705724</td>\n",
       "      <td>0.492510</td>\n",
       "      <td>-0.740348</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017233</td>\n",
       "      <td>-0.403057</td>\n",
       "      <td>0.584027</td>\n",
       "      <td>0.327661</td>\n",
       "      <td>-0.979785</td>\n",
       "      <td>0.101148</td>\n",
       "      <td>-1.658885</td>\n",
       "      <td>0.427704</td>\n",
       "      <td>0.683032</td>\n",
       "      <td>-0.845991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19936</th>\n",
       "      <td>-0.142877</td>\n",
       "      <td>1.278156</td>\n",
       "      <td>1.362879</td>\n",
       "      <td>-0.088607</td>\n",
       "      <td>-0.932164</td>\n",
       "      <td>1.060647</td>\n",
       "      <td>0.750132</td>\n",
       "      <td>0.468578</td>\n",
       "      <td>0.510927</td>\n",
       "      <td>-0.755303</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.119051</td>\n",
       "      <td>-1.250715</td>\n",
       "      <td>0.399990</td>\n",
       "      <td>0.073706</td>\n",
       "      <td>-0.992488</td>\n",
       "      <td>-0.206038</td>\n",
       "      <td>-1.285637</td>\n",
       "      <td>0.439310</td>\n",
       "      <td>1.039929</td>\n",
       "      <td>-0.753452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19937 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.285187  1.035191  1.682350  0.104763 -0.882211 -0.137103 -0.170671   \n",
       "1      0.415295 -0.219920  1.657734  0.139547 -0.228371  0.232568  0.100882   \n",
       "2      0.627834  0.471727  1.673070 -0.047936 -0.804701 -0.305792 -0.144297   \n",
       "3      0.458096  0.393705  1.368870 -0.532301 -0.976260 -0.014768 -0.083186   \n",
       "4     -0.406970 -0.212999  0.801990 -0.071332 -0.990373 -0.058411  0.054665   \n",
       "5      0.879064  0.530055  1.249226 -0.306988 -0.174719 -0.108372 -0.099921   \n",
       "6      0.606144  0.481296  1.404062  0.297868 -0.596131 -0.033229 -0.073326   \n",
       "7      0.758421  0.735004  1.905728 -0.301664 -0.361610  0.061511 -0.408666   \n",
       "8      0.226021  0.150854  1.476324 -0.401378 -1.471908  0.346949 -0.560114   \n",
       "9      0.494343  0.050526  0.829025  0.259778 -1.530470  0.309950  0.236543   \n",
       "10     0.705172  0.154084  2.135681 -0.404793 -0.670155  0.238828 -0.116875   \n",
       "11     0.298634  0.614322  1.764872 -0.182893 -0.485984 -0.490811 -0.278629   \n",
       "12    -0.748262  0.159923  1.081423  0.203167 -1.023936  1.107684  0.017704   \n",
       "13    -0.611335  0.290104  1.193328  0.226539 -0.640593  0.892652  0.190626   \n",
       "14    -0.324430 -0.147560  1.246451  0.125810 -0.828405  1.298855 -0.080084   \n",
       "15    -0.116764  0.251204  1.590876 -0.019090 -1.101640  1.118139 -0.165261   \n",
       "16    -0.279634  0.197268  1.116551  0.057708 -1.350899  0.468304 -0.336921   \n",
       "17     0.092136  0.255844  1.622497  0.471705 -1.937269  1.571148  0.277214   \n",
       "18     0.344009 -1.119431  1.206854  0.114018 -1.561882  1.360267  0.475750   \n",
       "19    -0.346075  0.736056  1.842471  0.560988 -1.089234  0.353329 -0.178329   \n",
       "20    -0.056211  0.043081  1.640319  0.356446 -0.652944  0.714529  0.318359   \n",
       "21     0.422841 -0.373147  0.864455  0.435885 -2.057341  1.404463  0.661329   \n",
       "22    -0.244289 -0.666319  1.469011  0.285309 -0.981507  1.232740  0.299815   \n",
       "23     0.153335 -0.327218  1.588007  0.176319 -1.989055  0.961840  0.148000   \n",
       "24     0.603135  0.070254  1.621128  0.840278 -0.478582 -0.015553  0.541238   \n",
       "25    -0.242318 -0.696743  1.178905  0.282141 -1.531175  1.029963  0.463092   \n",
       "26     0.681934 -0.517908  0.801893  0.651121 -0.818573  1.394127  0.330933   \n",
       "27     0.702217 -0.970979  1.065473  0.568281 -1.581719  1.438031  1.089776   \n",
       "28     0.407817 -0.562328  1.426271  0.317435 -2.243041  1.521384  0.540068   \n",
       "29     0.134201 -0.700121  1.145826  0.345384 -1.634236  0.964431 -0.114421   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19907  0.174146  0.824128  0.992666 -0.030849 -1.412738  0.595410 -0.309645   \n",
       "19908  0.695981  0.826697  1.224926 -0.109624 -0.616410 -0.048770  0.624648   \n",
       "19909  0.568925  0.557675  0.242264  0.564490 -0.899195  0.752759  0.355311   \n",
       "19910 -0.325505  0.006913 -0.167568 -0.043454 -1.122498  0.032773 -0.138201   \n",
       "19911  0.483715 -0.621755  0.487636 -0.716280 -1.833009  0.406727  0.329624   \n",
       "19912 -0.876192 -0.155553  0.035603 -0.788664 -0.875220 -0.180997 -0.361549   \n",
       "19913 -0.613963  0.056403  0.227106  0.154741 -0.837306 -0.238049  0.920116   \n",
       "19914 -0.301567 -0.295941  0.263274 -0.240122 -1.063000 -0.147889  0.263024   \n",
       "19915 -0.546964  0.037918  0.138908 -0.085343 -0.792171  0.305971  0.298391   \n",
       "19916 -0.525858  0.249089  0.606795  0.548324 -1.893947  0.271687  0.001447   \n",
       "19917 -0.340857 -0.400098  1.023823  0.511328 -1.514218  0.123152 -0.192541   \n",
       "19918 -0.075984 -0.481886  0.251686  0.238351 -1.393104 -0.089930 -0.015873   \n",
       "19919 -0.163019 -0.393016  0.874340 -0.606862 -1.515917 -0.302979  0.702096   \n",
       "19920 -0.347073  0.210814  0.367967  0.065793 -1.145852  0.092000 -0.321794   \n",
       "19921  0.011231  0.086537  0.680752 -0.944560 -1.150014  0.373362 -0.151931   \n",
       "19922 -0.708079  0.214840  0.248001  0.887576 -1.118023  0.191037  0.698078   \n",
       "19923 -0.057824  0.011236  0.425260  0.267986 -0.960930 -0.342984 -0.158363   \n",
       "19924 -1.128611  0.465557  0.194950  0.110801 -1.225296  0.207692  0.134170   \n",
       "19925 -0.631955  0.144171  0.493337 -0.444009 -0.669438  0.163358  0.202987   \n",
       "19926 -0.350087  0.504502  0.277488 -0.594621 -0.939992  0.326250 -0.137488   \n",
       "19927 -0.548712  0.045855  0.479962 -0.354462 -0.812022 -0.062447  0.165850   \n",
       "19928 -0.103767 -0.268599  0.354089  0.103874 -1.339020 -0.405641 -0.020696   \n",
       "19929  0.432389 -0.993138  1.169832  0.448713 -1.649643  1.357875  0.610546   \n",
       "19930 -0.015565 -0.463616  0.786974  0.794398 -0.818871  1.302955 -0.004605   \n",
       "19931 -0.364753  0.179435  0.323016 -1.015851 -0.369367 -0.031561 -0.254441   \n",
       "19932  0.015063 -0.592425  1.442457  0.201672 -1.745115  1.081401  0.665293   \n",
       "19933  0.128494  0.520338  1.358082 -0.635189 -0.499592 -0.432534  0.027534   \n",
       "19934 -0.135650 -0.933318  1.163087 -0.009839 -1.472165  0.873699  0.784119   \n",
       "19935  0.571342  0.572098  1.033618  0.453356 -0.800633  1.801874  0.459235   \n",
       "19936 -0.142877  1.278156  1.362879 -0.088607 -0.932164  1.060647  0.750132   \n",
       "\n",
       "            7         8         9      ...          250       251       252  \\\n",
       "0      1.225717  0.479215 -0.046635    ...    -0.151620 -1.350320  0.321935   \n",
       "1      0.351741  1.133607 -0.896950    ...     0.039406 -0.948139 -0.052438   \n",
       "2      1.070119  1.287865 -0.244582    ...     0.692564 -1.668460  0.260366   \n",
       "3      1.086714  1.146637 -0.849305    ...     0.524199 -1.173215  0.617650   \n",
       "4      0.953116  0.228877 -0.635901    ...     0.891404 -0.702635 -0.048356   \n",
       "5      0.928785  1.009993 -0.044301    ...     0.141253 -1.033423  0.320357   \n",
       "6      0.941797  0.974575 -0.153837    ...     0.496372 -1.701310  0.067612   \n",
       "7      0.946330  1.040725 -0.766435    ...    -0.417399 -1.496179  0.238060   \n",
       "8      1.153265  0.853556 -0.379513    ...     0.057440 -0.719270 -0.867954   \n",
       "9      0.625032  1.115580 -0.925240    ...     0.724463 -0.787428 -0.091150   \n",
       "10     1.061091  2.077640 -0.113839    ...     0.446646 -0.921451 -0.266021   \n",
       "11     1.056378  0.643862 -0.494605    ...     0.212744 -1.089869  0.304487   \n",
       "12     0.499456  0.134574 -0.628260    ...     0.822763 -0.299953 -0.376747   \n",
       "13     0.681198  0.267802 -0.449859    ...     0.664565 -0.396488 -0.096179   \n",
       "14     0.907570  0.282221 -0.480228    ...     0.392841 -0.615788  0.187657   \n",
       "15     0.612909  0.365554 -0.381981    ...     0.430700 -0.218972 -0.255822   \n",
       "16     0.640288  0.890486 -0.227668    ...     0.723747 -0.589558 -0.748660   \n",
       "17     1.470943  0.521168 -0.133855    ...     0.134251 -0.882997 -0.349122   \n",
       "18     0.919540  0.387768 -0.678120    ...     1.087092 -0.818499  0.033627   \n",
       "19     0.086533  0.274926 -0.168760    ...    -0.243486 -0.841205  0.203325   \n",
       "20     0.902505  0.410563 -0.411312    ...     0.876566 -0.244997 -0.213275   \n",
       "21     1.482676  0.012479 -0.994905    ...     0.391724 -0.641041 -0.257436   \n",
       "22     0.782959  0.123607 -0.977749    ...     0.525912 -0.981852  0.470226   \n",
       "23     1.145125  0.691538 -0.680728    ...     0.424098 -0.162036  0.109003   \n",
       "24     1.157969  0.192505 -0.064892    ...     0.961037  0.010108  0.375949   \n",
       "25     1.274598 -0.090282 -1.123560    ...     0.925937 -0.553136  0.473705   \n",
       "26     1.229115 -0.166330 -0.831579    ...     0.921571 -0.862533  0.719016   \n",
       "27     0.715492  0.571947 -0.630926    ...     0.781380 -0.977994  0.460930   \n",
       "28     0.874356 -0.457330 -0.840793    ...     0.441035 -0.292092  0.489717   \n",
       "29     1.492166  0.572583 -0.868971    ...     0.890958 -0.399293  0.544484   \n",
       "...         ...       ...       ...    ...          ...       ...       ...   \n",
       "19907  0.801669  0.920571 -0.583945    ...     0.126044 -1.298162  0.138345   \n",
       "19908  0.829235  0.343069 -0.817465    ...     0.112368 -0.934944  0.871879   \n",
       "19909  1.137420  0.184258 -0.779983    ...     0.382551 -1.071041  0.661890   \n",
       "19910  1.584826 -0.175997  0.755036    ...     0.930859 -0.170287  0.146898   \n",
       "19911  0.610548 -0.020123  1.221650    ...     0.568632 -0.791973  0.585378   \n",
       "19912  1.416749 -0.281474  0.311666    ...     1.126562  0.622560 -0.030787   \n",
       "19913  0.827699 -0.235098  0.352090    ...     0.919164 -0.277150 -0.362255   \n",
       "19914  0.479348 -0.169673 -0.087236    ...     1.011245 -0.137989 -1.150331   \n",
       "19915  1.695459  0.352275  0.112681    ...     0.694319  0.175414 -0.287363   \n",
       "19916  1.294056 -0.041803 -0.360426    ...     0.783156  0.490598 -0.494018   \n",
       "19917  1.516669  0.099132 -0.112112    ...     0.876835  0.363048  0.099218   \n",
       "19918  1.253157  0.079276  0.033953    ...     0.630640  0.381187  0.061986   \n",
       "19919  1.607521 -0.395949  0.501853    ...     1.046992  0.388692  0.304665   \n",
       "19920  1.519922  0.145669  0.321942    ...     0.227727  0.202683 -0.039522   \n",
       "19921  1.472334  0.040101 -0.554558    ...     1.099976 -0.168161 -1.033424   \n",
       "19922  1.639054 -0.326507 -0.240289    ...     0.327596  0.357065 -0.069434   \n",
       "19923  1.308088 -0.377870  0.227946    ...     0.437802  0.456166 -0.318535   \n",
       "19924  1.903555 -0.244991  0.677518    ...     0.421385  0.501519 -0.207746   \n",
       "19925  1.073492  0.599670 -0.343284    ...     0.444883  0.666261  0.178824   \n",
       "19926  1.069369  0.392929  0.631852    ...     0.656610  0.117592 -0.298384   \n",
       "19927  2.033219 -0.262042  0.389151    ...     0.945431  0.290358  0.119895   \n",
       "19928  2.081672 -0.295670  0.621130    ...     0.860067  0.363640 -0.312848   \n",
       "19929  0.970406  0.325847 -0.786682    ...     0.845192 -0.495958  0.160524   \n",
       "19930  0.551007  0.340697 -1.377090    ...     1.077384 -0.487117  0.248811   \n",
       "19931  1.076610  1.221405 -0.745442    ...     0.609532 -0.957707 -0.318597   \n",
       "19932  1.004689 -0.170649 -0.982375    ...     0.909010 -0.504377  0.332382   \n",
       "19933  1.110284  0.104205  0.208385    ...     0.618037  0.519769  1.411446   \n",
       "19934  0.950420 -0.050132 -1.339624    ...     1.219578 -0.185845  0.121810   \n",
       "19935  0.705724  0.492510 -0.740348    ...    -0.017233 -0.403057  0.584027   \n",
       "19936  0.468578  0.510927 -0.755303    ...    -0.119051 -1.250715  0.399990   \n",
       "\n",
       "            253       254       255       256       257       258       259  \n",
       "0     -0.611737  0.164607  0.189246 -1.987858  0.204009  0.415615 -1.623930  \n",
       "1     -0.566592 -0.314158 -0.007940 -1.963288 -0.075500  0.747609 -0.833278  \n",
       "2     -0.797651  0.439347  0.235958 -1.521491  0.536750  0.221807 -1.815555  \n",
       "3     -1.194660  0.048380  0.426114 -1.373202  0.501936  0.940675 -1.607249  \n",
       "4     -0.515967 -0.334811  0.770778 -0.632434 -0.130604  0.713931 -0.288358  \n",
       "5     -0.758868  0.304653  0.542335 -1.863190  0.382143  0.461409 -0.911531  \n",
       "6     -0.826262  0.641773 -0.028948 -1.980331  0.458563  0.286968 -1.450641  \n",
       "7     -0.713422  0.028800  0.191091 -1.666864  0.187643  0.942366 -1.543681  \n",
       "8     -0.089173 -0.595668  0.394974 -1.161239 -0.098599  0.909504 -0.683668  \n",
       "9      0.406892 -0.781916 -0.022643 -1.317075  0.262037 -0.102802 -1.503233  \n",
       "10    -0.324262 -0.567364  0.726553 -1.432780  0.484867  0.625350 -1.499931  \n",
       "11    -0.164915 -0.479554  0.806182 -1.731338  0.129312  0.580873 -1.873682  \n",
       "12    -0.579324 -0.578580 -0.185159 -0.512545  0.448680  0.254131 -0.699271  \n",
       "13    -0.314231 -0.359714 -0.085456 -0.882273  0.297162  0.293442 -0.685459  \n",
       "14    -0.501334 -0.350578 -0.070487 -0.657295 -0.002836  0.408546 -0.865078  \n",
       "15    -0.404894 -0.321728  0.305401 -1.030368  0.597390  0.597561 -0.277033  \n",
       "16    -0.312159 -0.657484  0.029896 -1.169107  0.012457  0.836292 -0.268422  \n",
       "17    -0.360120 -1.010623  0.638444 -0.793697  0.026867  0.542373 -0.751735  \n",
       "18    -0.518780 -0.371036  0.688531 -0.427074  0.124376  0.458212 -0.040441  \n",
       "19    -0.195250 -0.451563 -0.469346 -1.979240 -0.025462  0.655013 -0.851254  \n",
       "20    -0.418808 -1.141672 -0.295635 -1.696517  0.097755  0.169520 -0.631464  \n",
       "21    -0.352003 -0.802625  1.069941 -0.304242 -0.399835  0.844856  0.038447  \n",
       "22    -0.519586 -0.213811  0.358933 -0.439318 -0.066122 -0.166237 -0.120940  \n",
       "23    -0.951403 -0.986287  0.744978 -0.361658  0.239314  0.502896 -0.985339  \n",
       "24    -0.259983 -1.506536  0.517542 -1.463983 -0.281955 -0.590905 -1.198447  \n",
       "25    -0.289715 -0.716616  0.612532  0.000611  0.029923 -0.172505 -0.109875  \n",
       "26     0.269031 -0.827607  0.131635 -0.954797  0.333096 -0.148470 -0.621245  \n",
       "27    -0.543460 -0.249600  0.293791 -0.532622  0.555038  0.267074 -0.661679  \n",
       "28    -0.509199 -0.607702  0.980750  0.010450 -0.286908  0.355200 -0.144650  \n",
       "29     0.113059 -0.739818  0.757374 -0.015978  0.160122 -0.081263 -0.466850  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "19907  0.057977 -0.735195 -0.042736 -1.629146  0.596539  0.149328 -1.258806  \n",
       "19908  0.183696 -0.068529  0.243585 -1.985366  0.284895 -0.025500 -1.064124  \n",
       "19909 -0.046969 -0.902392  0.394801 -1.293081 -0.147650  0.531089 -1.026419  \n",
       "19910 -0.821167 -0.649653 -0.028856  0.617293 -0.157290  0.004324 -0.174713  \n",
       "19911 -0.761671 -0.782381  0.362813  0.952196  0.722977  0.453143 -1.116172  \n",
       "19912 -1.036571 -1.407478  1.109743  0.482682 -0.136174  0.779005 -0.038232  \n",
       "19913 -0.542223 -1.032610  0.406903  0.728829  0.280514  0.012955 -0.415873  \n",
       "19914 -0.324849 -2.330701  1.025317  1.026803 -0.111122  0.836173 -0.356987  \n",
       "19915 -0.821994 -1.523808  0.748204  0.318738 -0.763715  0.959687 -1.096544  \n",
       "19916 -0.150094 -2.434743  1.183848  0.805000  0.277010  1.216865 -0.994504  \n",
       "19917 -0.074914 -1.664705  0.439400 -0.102686  0.215025  0.753520 -2.192087  \n",
       "19918 -0.648451 -0.872256  0.070370 -0.107316 -0.297673  0.733173 -1.506285  \n",
       "19919 -0.124389 -1.269499  0.438265  0.575130 -0.260324 -0.113101 -1.719341  \n",
       "19920 -0.809958 -1.490975  0.453533  0.006769 -0.660903  0.381508 -0.909558  \n",
       "19921 -1.207961 -1.421215  1.299597  0.753097 -1.069383  0.935988 -0.570107  \n",
       "19922 -1.036808 -1.304366  0.303576 -0.248055 -0.367064  0.655427 -1.266412  \n",
       "19923 -0.145937 -1.667207  0.407271 -0.281128 -0.007691  0.524799 -1.192680  \n",
       "19924 -0.252077 -0.914632  0.106953  0.269106 -0.974446  0.800509 -1.633075  \n",
       "19925 -0.770191 -0.144058  0.829824 -0.099973  0.131971  0.516922 -1.129611  \n",
       "19926 -1.435194 -0.986214  0.699533  0.288776 -0.193862  0.339595 -0.696602  \n",
       "19927  0.455461 -1.029226  0.709587  0.512314 -0.077732  0.857862 -1.561407  \n",
       "19928 -0.393726 -1.915523  0.539097  0.922638  0.305432  0.361016 -0.756502  \n",
       "19929 -0.721510 -0.444435  0.594968 -0.398151  0.271303  0.219992 -0.327732  \n",
       "19930  0.172261 -0.881916  0.018200 -0.744332  0.762182  0.744539 -0.961440  \n",
       "19931  0.035867 -0.517285  1.049349 -0.238038 -0.414994  0.437721  0.104299  \n",
       "19932 -0.353062 -0.590654  0.368110 -0.337343  0.311092  0.412479 -0.193659  \n",
       "19933 -0.344667 -0.953240  0.335714 -0.527777 -0.012892 -0.045325 -0.294871  \n",
       "19934 -0.085776 -0.999106 -0.037240 -0.692400  0.448869  0.400963 -0.139010  \n",
       "19935  0.327661 -0.979785  0.101148 -1.658885  0.427704  0.683032 -0.845991  \n",
       "19936  0.073706 -0.992488 -0.206038 -1.285637  0.439310  1.039929 -0.753452  \n",
       "\n",
       "[19937 rows x 260 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z + pd.concat([pd.DataFrame(neural_network[0].bias)]*z.shape[0], ignore_index=True) # Ignores the index\n",
    "#z+b.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wifi_columns = df_train.columns[:520]\n",
    "# Transformation Function\n",
    "df_train[wifi_columns] = df_train[wifi_columns].apply(lambda x: 100-x, axis=1) # change range to [0, ...]\n",
    "\n",
    "\n",
    "# euclidean distance  \n",
    "#    d <- sqrt( sum( (xtest[i,]-xtrain[j,])^2 ) )\n",
    "output = ['BUILDINGID', 'FLOOR', 'LATITUDE', 'LONGITUDE']\n",
    "building = 0\n",
    "floor = 1\n",
    "latitude = 2\n",
    "longitude = 3\n",
    "#output[building]\n",
    "\n",
    "wifi_columns = df_train.columns[:520]\n",
    "x_train = df_train[wifi_columns]\n",
    "y_train = df_train[output[building]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "classifier_model = LogisticRegression()\n",
    "regression_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimadorDictionary = getClassifierModels(includeEnsambled=True)\n",
    "modelNameList = getClassifierNames(includeEnsambled=True)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=False)\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def accert(y_true, y_pred): \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return (cm.diagonal()/cm.sum(0)).mean()\n",
    "\n",
    "scoring_acc = {\n",
    "    #'average_precision' : 'average_precision_weighted',\n",
    "    #'precision': 'precision',\n",
    "    #'recall': 'recall',\n",
    "    #'balanced_accuracy': 'balanced_accuracy',\n",
    "    #'roc_auc': 'roc_auc',\n",
    "    'nbaccuracy' : make_scorer(accert),\n",
    "    'accuracy': 'accuracy'\n",
    "}\n",
    "\n",
    "\n",
    "#estimador = classifier_model\n",
    "#modelName = str(estimador).split('(')[0]\n",
    "result_acc = cross_validate(classifier_model, x_train, y_train, scoring=scoring_acc, cv=kf, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.275247</td>\n",
       "      <td>0.268692</td>\n",
       "      <td>0.311988</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.343994</td>\n",
       "      <td>0.267918</td>\n",
       "      <td>0.234304</td>\n",
       "      <td>0.290608</td>\n",
       "      <td>0.244015</td>\n",
       "      <td>0.224418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.007994</td>\n",
       "      <td>0.009181</td>\n",
       "      <td>0.006175</td>\n",
       "      <td>0.012188</td>\n",
       "      <td>0.005842</td>\n",
       "      <td>0.005437</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.005446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_nbaccuracy</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.998801</td>\n",
       "      <td>0.983828</td>\n",
       "      <td>0.999072</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_accuracy</th>\n",
       "      <td>0.987462</td>\n",
       "      <td>0.999498</td>\n",
       "      <td>0.984453</td>\n",
       "      <td>0.998997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998495</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0         1         2         3         4         5  \\\n",
       "fit_time         0.275247  0.268692  0.311988  0.240143  0.343994  0.267918   \n",
       "score_time       0.007994  0.009181  0.006175  0.012188  0.005842  0.005437   \n",
       "test_nbaccuracy       NaN  0.998801  0.983828  0.999072  1.000000  1.000000   \n",
       "test_accuracy    0.987462  0.999498  0.984453  0.998997  1.000000  1.000000   \n",
       "\n",
       "                        6         7         8         9  \n",
       "fit_time         0.234304  0.290608  0.244015  0.224418  \n",
       "score_time       0.010417  0.005345  0.005376  0.005446  \n",
       "test_nbaccuracy  1.000000  0.999008  1.000000  0.997393  \n",
       "test_accuracy    1.000000  0.998495  1.000000  0.995986  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result_acc['test_accuracy'].mean()\n",
    "pd.DataFrame(result_acc).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as scoreMetrics\n",
    "import geopy.distance\n",
    "from functools import reduce\n",
    "\n",
    "#x_trainReg = pd.concat([X_train, y_train[['BUILDINGID']]], axis=1) # , 'FLOOR'\n",
    "\n",
    "_meanLat = 39.9926853\n",
    "_meanLon = -0.0673033\n",
    "_minLongitude = -7705\n",
    "_maxLongitude = -7290\n",
    "_minLatitude = 4864735\n",
    "_maxLatitude = 4865023\n",
    "_maxLatitudeGPS = 39.993720\n",
    "_maxLongitudeGPS = -0.069254\n",
    "_minLatitudeGPS = 39.991626\n",
    "_minLongitudeGPS = -0.065425\n",
    "\n",
    "def longitudeToGPS(x):\n",
    "    return (_maxLongitudeGPS - _minLongitudeGPS) * (x - _minLongitude) / (_maxLongitude - _minLongitude) + _minLongitudeGPS\n",
    "\n",
    "def latitudeToGPS(x):\n",
    "    return (_maxLatitudeGPS - _minLatitudeGPS) * (x - _minLatitude) / (_maxLatitude - _minLatitude) + _minLatitudeGPS\n",
    "\n",
    "def latitudeListDistance(y_true, y_pred):\n",
    "    return list(map(lambda yt,yp : geopy.distance.vincenty((_meanLon, yt),(_meanLon, yp)).m , latitudeToGPS(y_true), latitudeToGPS(y_pred)))\n",
    "\n",
    "def longitudeListDistance(y_true, y_pred):\n",
    "    return list(map(lambda yt,yp : geopy.distance.vincenty((yt, _meanLat),(yp, _meanLat)).m , longitudeToGPS(y_true), longitudeToGPS(y_pred)))\n",
    "    \n",
    "def distance2d(y_true, y_pred):\n",
    "    ldis = []\n",
    "    if ((y_true>0).sum()>0):\n",
    "        #ldis = list(map(lambda yt,yp : geopy.distance.vincenty((_meanLon, yt),(_meanLon, yp)).m , latitudeToGPS(y_true), latitudeToGPS(y_pred)))\n",
    "        ldis = latitudeListDistance(y_true, y_pred)\n",
    "    else:\n",
    "        #ldis = list(map(lambda yt,yp : geopy.distance.vincenty((yt, _meanLat),(yp, _meanLat)).m , longitudeToGPS(y_true), longitudeToGPS(y_pred)))\n",
    "        ldis = longitudeListDistance(y_true, y_pred)\n",
    "    return reduce(lambda x,y: x+y, ldis) / len(ldis)\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return scoreMetrics.mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "def mae(y_true, y_pred):\n",
    "    return scoreMetrics.mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "scoring_reg = {\n",
    "    'mae': make_scorer(mae),# 'mean_absolute_error',\n",
    "    'mse': make_scorer(mse),#'mean_squared_error',\n",
    "    'distance': make_scorer(distance2d),\n",
    "    'r2': 'r2'\n",
    "}\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "result = cross_validate(RandomForestRegressor(), x_train, df_train[output[latitude]], scoring=scoring_reg, cv=kf, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132.70542971354755"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['test_mse'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scoring_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2245d30afcef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregression_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scoring_reg' is not defined"
     ]
    }
   ],
   "source": [
    "result = cross_validate(regression_model, x_train, y_train, scoring=scoring_reg, cv=kf, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import is_classifier\n",
    "is_classifier(regression_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>3.310302</td>\n",
       "      <td>3.276414</td>\n",
       "      <td>3.206123</td>\n",
       "      <td>3.447507</td>\n",
       "      <td>3.969715</td>\n",
       "      <td>3.946906</td>\n",
       "      <td>3.821521</td>\n",
       "      <td>3.306685</td>\n",
       "      <td>3.727673</td>\n",
       "      <td>3.554831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.060155</td>\n",
       "      <td>0.056511</td>\n",
       "      <td>0.057894</td>\n",
       "      <td>0.068151</td>\n",
       "      <td>0.059055</td>\n",
       "      <td>0.092192</td>\n",
       "      <td>0.056778</td>\n",
       "      <td>0.061817</td>\n",
       "      <td>0.057405</td>\n",
       "      <td>0.062549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_mae</th>\n",
       "      <td>11.999786</td>\n",
       "      <td>8.382823</td>\n",
       "      <td>8.553397</td>\n",
       "      <td>4.531076</td>\n",
       "      <td>3.708259</td>\n",
       "      <td>4.153408</td>\n",
       "      <td>7.015856</td>\n",
       "      <td>5.832119</td>\n",
       "      <td>2.504778</td>\n",
       "      <td>5.449663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_mse</th>\n",
       "      <td>374.018346</td>\n",
       "      <td>175.070800</td>\n",
       "      <td>312.779440</td>\n",
       "      <td>54.068660</td>\n",
       "      <td>36.673099</td>\n",
       "      <td>46.322053</td>\n",
       "      <td>116.534210</td>\n",
       "      <td>98.546306</td>\n",
       "      <td>30.442981</td>\n",
       "      <td>82.598402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_distance</th>\n",
       "      <td>9.712432</td>\n",
       "      <td>6.784916</td>\n",
       "      <td>6.922975</td>\n",
       "      <td>3.667368</td>\n",
       "      <td>3.001390</td>\n",
       "      <td>3.361689</td>\n",
       "      <td>5.678515</td>\n",
       "      <td>4.720414</td>\n",
       "      <td>2.027314</td>\n",
       "      <td>4.410860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_r2</th>\n",
       "      <td>0.893941</td>\n",
       "      <td>0.877378</td>\n",
       "      <td>0.852526</td>\n",
       "      <td>0.984421</td>\n",
       "      <td>0.991170</td>\n",
       "      <td>0.991761</td>\n",
       "      <td>0.939706</td>\n",
       "      <td>0.950103</td>\n",
       "      <td>0.971248</td>\n",
       "      <td>0.980652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0           1           2          3          4  \\\n",
       "fit_time         3.310302    3.276414    3.206123   3.447507   3.969715   \n",
       "score_time       0.060155    0.056511    0.057894   0.068151   0.059055   \n",
       "test_mae        11.999786    8.382823    8.553397   4.531076   3.708259   \n",
       "test_mse       374.018346  175.070800  312.779440  54.068660  36.673099   \n",
       "test_distance    9.712432    6.784916    6.922975   3.667368   3.001390   \n",
       "test_r2          0.893941    0.877378    0.852526   0.984421   0.991170   \n",
       "\n",
       "                       5           6          7          8          9  \n",
       "fit_time        3.946906    3.821521   3.306685   3.727673   3.554831  \n",
       "score_time      0.092192    0.056778   0.061817   0.057405   0.062549  \n",
       "test_mae        4.153408    7.015856   5.832119   2.504778   5.449663  \n",
       "test_mse       46.322053  116.534210  98.546306  30.442981  82.598402  \n",
       "test_distance   3.361689    5.678515   4.720414   2.027314   4.410860  \n",
       "test_r2         0.991761    0.939706   0.950103   0.971248   0.980652  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result).T#.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Distribución de data 80% data para entrenamiento y 20% para validación__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "seed = 9\n",
    "xSize = 1055\n",
    "kf = KFold(n_splits=10)\n",
    "df = pd.read_csv(\"data/filtred.csv\")\n",
    "X = df[df.columns[:xSize]]\n",
    "Y = df[df.columns[xSize:]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "gbTrain = GraphicBuilder(pd.concat([X_train, y_train],axis=1))\n",
    "gbTest = GraphicBuilder(pd.concat([X_test, y_test],axis=1))\n",
    "# x_trainReg = pd.concat([X, Y[['BUILDINGID', 'FLOOR']]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_building = []\n",
    "l_floor = []\n",
    "l_latitude = []\n",
    "l_longitude = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>2.272874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.105477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_mae</th>\n",
       "      <td>7.024801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_mse</th>\n",
       "      <td>89.337374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_distance</th>\n",
       "      <td>5.685750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_r2</th>\n",
       "      <td>0.980692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               LinearRegression\n",
       "fit_time               2.272874\n",
       "score_time             0.105477\n",
       "test_mae               7.024801\n",
       "test_mse              89.337374\n",
       "test_distance          5.685750\n",
       "test_r2                0.980692"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pd.DataFrame(scoreLat).mean(), columns=[modelName])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LogisticRegression',\n",
       " 'SGDClassifier',\n",
       " 'PassiveAggressiveClassifier',\n",
       " 'MLPClassifier',\n",
       " 'LinearDiscriminantAnalysis',\n",
       " 'QuadraticDiscriminantAnalysis',\n",
       " 'KNeighborsClassifier',\n",
       " 'DecisionTreeClassifier',\n",
       " 'GaussianNB',\n",
       " 'BernoulliNB',\n",
       " 'MultinomialNB',\n",
       " 'SVC',\n",
       " 'AdaBoostClassifier',\n",
       " 'GradientBoostingClassifier',\n",
       " 'RandomForestClassifier',\n",
       " 'ExtraTreesClassifier',\n",
       " 'VotingClassifier',\n",
       " 'BaggingClassifier']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getClassifierNames(includeEnsambled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLatitude = pd.DataFrame(pd.DataFrame(l_latitude[0]).mean(), columns=['LinearRegression'])\n",
    "dfLatitude['Lasso'] = pd.DataFrame(pd.DataFrame(l_latitude[1]).mean())\n",
    "dfLatitude['Ridge'] = pd.DataFrame(pd.DataFrame(l_latitude[2]).mean())\n",
    "dfLatitude['ElasticNet'] = pd.DataFrame(pd.DataFrame(l_latitude[3]).mean())\n",
    "dfLatitude['PassiveAggressiveRegressor'] = pd.DataFrame(pd.DataFrame(l_latitude[4]).mean())\n",
    "dfLatitude['SVR'] = pd.DataFrame(pd.DataFrame(l_latitude[5]).mean())\n",
    "dfLatitude['DecisionTreeRegressor'] = pd.DataFrame(pd.DataFrame(l_latitude[6]).mean())\n",
    "dfLatitude['KNeighborsRegressor'] = pd.DataFrame(pd.DataFrame(l_latitude[7]).mean())\n",
    "dfLatitude['GaussianProcessRegressor'] = pd.DataFrame(pd.DataFrame(l_latitude[8]).mean())\n",
    "dfLatitude['AdaBoostRegressor'] = pd.DataFrame(pd.DataFrame(l_latitude[9]).mean())\n",
    "dfLatitude['GradientBoostingRegressor'] = pd.DataFrame(pd.DataFrame(l_latitude[10]).mean())\n",
    "dfLatitude['RandomForestRegressor'] = pd.DataFrame(pd.DataFrame(l_latitude[11]).mean())\n",
    "dfLatitude['ExtraTreesRegressor'] = pd.DataFrame(pd.DataFrame(l_latitude[12]).mean())\n",
    "dfLatitude['BaggingRegressor'] = pd.DataFrame(pd.DataFrame(l_latitude[13]).mean())\n",
    "dfLatitude.to_csv(\"result/_latitude/Comparacion.csv\", index=False)\n",
    "\n",
    "dfLongitude = pd.DataFrame(pd.DataFrame(l_longitude[0]).mean(), columns=['LinearRegression'])\n",
    "dfLongitude['Lasso'] = pd.DataFrame(pd.DataFrame(l_longitude[1]).mean())\n",
    "dfLongitude['Ridge'] = pd.DataFrame(pd.DataFrame(l_longitude[2]).mean())\n",
    "dfLongitude['ElasticNet'] = pd.DataFrame(pd.DataFrame(l_longitude[3]).mean())\n",
    "dfLongitude['PassiveAggressiveRegressor'] = pd.DataFrame(pd.DataFrame(l_longitude[4]).mean())\n",
    "dfLongitude['SVR'] = pd.DataFrame(pd.DataFrame(l_longitude[5]).mean())\n",
    "dfLongitude['DecisionTreeRegressor'] = pd.DataFrame(pd.DataFrame(l_longitude[6]).mean())\n",
    "dfLongitude['KNeighborsRegressor'] = pd.DataFrame(pd.DataFrame(l_longitude[7]).mean())\n",
    "dfLongitude['GaussianProcessRegressor'] = pd.DataFrame(pd.DataFrame(l_longitude[8]).mean())\n",
    "dfLongitude['AdaBoostRegressor'] = pd.DataFrame(pd.DataFrame(l_longitude[9]).mean())\n",
    "dfLongitude['GradientBoostingRegressor'] = pd.DataFrame(pd.DataFrame(l_longitude[10]).mean())\n",
    "dfLongitude['RandomForestRegressor'] = pd.DataFrame(pd.DataFrame(l_longitude[11]).mean())\n",
    "dfLongitude['ExtraTreesRegressor'] = pd.DataFrame(pd.DataFrame(l_longitude[12]).mean())\n",
    "dfLongitude['BaggingRegressor'] = pd.DataFrame(pd.DataFrame(l_longitude[13]).mean())\n",
    "dfLongitude.to_csv(\"result/_longitude/Comparacion.csv\", index=False)\n",
    "\n",
    "dfFloor = pd.DataFrame(pd.DataFrame(l_floor[0]).mean(), columns=['LogisticRegression'])\n",
    "dfFloor['SGDClassifier'] = pd.DataFrame(pd.DataFrame(l_floor[1]).mean())\n",
    "dfFloor['PassiveAggressiveClassifier'] = pd.DataFrame(pd.DataFrame(l_floor[2]).mean())\n",
    "dfFloor['MLPClassifier'] = pd.DataFrame(pd.DataFrame(l_floor[3]).mean())\n",
    "dfFloor['LinearDiscriminantAnalysis'] = pd.DataFrame(pd.DataFrame(l_floor[4]).mean())\n",
    "dfFloor['QuadraticDiscriminantAnalysis'] = pd.DataFrame(pd.DataFrame(l_floor[5]).mean())\n",
    "dfFloor['KNeighborsClassifier'] = pd.DataFrame(pd.DataFrame(l_floor[6]).mean())\n",
    "dfFloor['DecisionTreeClassifier'] = pd.DataFrame(pd.DataFrame(l_floor[7]).mean())\n",
    "dfFloor['GaussianNB'] = pd.DataFrame(pd.DataFrame(l_floor[8]).mean())\n",
    "dfFloor['BernoulliNB'] = pd.DataFrame(pd.DataFrame(l_floor[9]).mean())\n",
    "dfFloor['MultinomialNB'] = pd.DataFrame(pd.DataFrame(l_floor[10]).mean())\n",
    "dfFloor['SVC'] = pd.DataFrame(pd.DataFrame(l_floor[11]).mean())\n",
    "dfFloor['AdaBoostClassifier'] = pd.DataFrame(pd.DataFrame(l_floor[12]).mean())\n",
    "dfFloor['GradientBoostingClassifier'] = pd.DataFrame(pd.DataFrame(l_floor[13]).mean())\n",
    "dfFloor['RandomForestClassifier'] = pd.DataFrame(pd.DataFrame(l_floor[14]).mean())\n",
    "dfFloor['ExtraTreesClassifier'] = pd.DataFrame(pd.DataFrame(l_floor[15]).mean())\n",
    "dfFloor['VotingClassifier'] = pd.DataFrame(pd.DataFrame(l_floor[16]).mean())\n",
    "dfFloor['BaggingClassifier'] = pd.DataFrame(pd.DataFrame(l_floor[17]).mean())\n",
    "dfFloor.to_csv(\"result/_floor/Comparacion.csv\", index=False)\n",
    "\n",
    "dfBuilding = pd.DataFrame(pd.DataFrame(l_building[0]).mean(), columns=['LogisticRegression'])\n",
    "dfBuilding['SGDClassifier'] = pd.DataFrame(pd.DataFrame(l_building[1]).mean())\n",
    "dfBuilding['PassiveAggressiveClassifier'] = pd.DataFrame(pd.DataFrame(l_building[2]).mean())\n",
    "dfBuilding['MLPClassifier'] = pd.DataFrame(pd.DataFrame(l_building[3]).mean())\n",
    "dfBuilding['LinearDiscriminantAnalysis'] = pd.DataFrame(pd.DataFrame(l_building[4]).mean())\n",
    "dfBuilding['QuadraticDiscriminantAnalysis'] = pd.DataFrame(pd.DataFrame(l_building[5]).mean())\n",
    "dfBuilding['KNeighborsClassifier'] = pd.DataFrame(pd.DataFrame(l_building[6]).mean())\n",
    "dfBuilding['DecisionTreeClassifier'] = pd.DataFrame(pd.DataFrame(l_building[7]).mean())\n",
    "dfBuilding['GaussianNB'] = pd.DataFrame(pd.DataFrame(l_building[8]).mean())\n",
    "dfBuilding['BernoulliNB'] = pd.DataFrame(pd.DataFrame(l_building[9]).mean())\n",
    "dfBuilding['MultinomialNB'] = pd.DataFrame(pd.DataFrame(l_building[10]).mean())\n",
    "dfBuilding['SVC'] = pd.DataFrame(pd.DataFrame(l_building[11]).mean())\n",
    "dfBuilding['AdaBoostClassifier'] = pd.DataFrame(pd.DataFrame(l_building[12]).mean())\n",
    "dfBuilding['GradientBoostingClassifier'] = pd.DataFrame(pd.DataFrame(l_building[13]).mean())\n",
    "dfBuilding['RandomForestClassifier'] = pd.DataFrame(pd.DataFrame(l_building[14]).mean())\n",
    "dfBuilding['ExtraTreesClassifier'] = pd.DataFrame(pd.DataFrame(l_building[15]).mean())\n",
    "dfBuilding['VotingClassifier'] = pd.DataFrame(pd.DataFrame(l_building[16]).mean())\n",
    "dfBuilding['BaggingClassifier'] = pd.DataFrame(pd.DataFrame(l_building[17]).mean())\n",
    "dfBuilding.to_csv(\"result/_building/Comparacion.csv\", index=False)\n",
    "\n",
    "#pd.DataFrame(pd.DataFrame(l_building[0]).mean(), columns=[modelName])\n",
    "#l_floor\n",
    "#l_latitude\n",
    "#l_longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>Lasso</th>\n",
       "      <th>Ridge</th>\n",
       "      <th>ElasticNet</th>\n",
       "      <th>PassiveAggressiveRegressor</th>\n",
       "      <th>SVR</th>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <th>GaussianProcessRegressor</th>\n",
       "      <th>AdaBoostRegressor</th>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <th>BaggingRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>2.531144</td>\n",
       "      <td>1.054282</td>\n",
       "      <td>0.848534</td>\n",
       "      <td>0.898072</td>\n",
       "      <td>0.783983</td>\n",
       "      <td>348.231184</td>\n",
       "      <td>1.469077</td>\n",
       "      <td>1.982650</td>\n",
       "      <td>3.064316e+02</td>\n",
       "      <td>83.009644</td>\n",
       "      <td>38.564748</td>\n",
       "      <td>8.239344</td>\n",
       "      <td>9.545212</td>\n",
       "      <td>9.469980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.085233</td>\n",
       "      <td>0.081150</td>\n",
       "      <td>0.084967</td>\n",
       "      <td>0.084130</td>\n",
       "      <td>0.081812</td>\n",
       "      <td>154.376145</td>\n",
       "      <td>0.082450</td>\n",
       "      <td>31.751880</td>\n",
       "      <td>9.619912e+01</td>\n",
       "      <td>0.443865</td>\n",
       "      <td>0.166302</td>\n",
       "      <td>0.121974</td>\n",
       "      <td>0.135257</td>\n",
       "      <td>0.342275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_mae</th>\n",
       "      <td>8.540289</td>\n",
       "      <td>19.371092</td>\n",
       "      <td>8.507692</td>\n",
       "      <td>32.831996</td>\n",
       "      <td>158.575941</td>\n",
       "      <td>63.791369</td>\n",
       "      <td>2.782402</td>\n",
       "      <td>3.806230</td>\n",
       "      <td>3.801109e+03</td>\n",
       "      <td>2.061294</td>\n",
       "      <td>8.871777</td>\n",
       "      <td>2.764245</td>\n",
       "      <td>2.540180</td>\n",
       "      <td>2.742543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_mse</th>\n",
       "      <td>138.988203</td>\n",
       "      <td>552.791487</td>\n",
       "      <td>135.940109</td>\n",
       "      <td>1554.826630</td>\n",
       "      <td>36130.775816</td>\n",
       "      <td>6159.190611</td>\n",
       "      <td>56.103192</td>\n",
       "      <td>50.665884</td>\n",
       "      <td>2.235548e+07</td>\n",
       "      <td>24.625936</td>\n",
       "      <td>139.781715</td>\n",
       "      <td>30.824116</td>\n",
       "      <td>31.411138</td>\n",
       "      <td>30.021256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_distance</th>\n",
       "      <td>8.712924</td>\n",
       "      <td>19.762664</td>\n",
       "      <td>8.679669</td>\n",
       "      <td>33.495670</td>\n",
       "      <td>161.781433</td>\n",
       "      <td>65.080863</td>\n",
       "      <td>2.838646</td>\n",
       "      <td>3.883170</td>\n",
       "      <td>3.877946e+03</td>\n",
       "      <td>2.102961</td>\n",
       "      <td>9.051113</td>\n",
       "      <td>2.820122</td>\n",
       "      <td>2.591528</td>\n",
       "      <td>2.797981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_r2</th>\n",
       "      <td>0.991028</td>\n",
       "      <td>0.964329</td>\n",
       "      <td>0.991226</td>\n",
       "      <td>0.899752</td>\n",
       "      <td>-1.342505</td>\n",
       "      <td>0.602990</td>\n",
       "      <td>0.996383</td>\n",
       "      <td>0.996740</td>\n",
       "      <td>-1.441417e+03</td>\n",
       "      <td>0.998411</td>\n",
       "      <td>0.990982</td>\n",
       "      <td>0.998014</td>\n",
       "      <td>0.997974</td>\n",
       "      <td>0.998066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               LinearRegression       Lasso       Ridge   ElasticNet  \\\n",
       "fit_time               2.531144    1.054282    0.848534     0.898072   \n",
       "score_time             0.085233    0.081150    0.084967     0.084130   \n",
       "test_mae               8.540289   19.371092    8.507692    32.831996   \n",
       "test_mse             138.988203  552.791487  135.940109  1554.826630   \n",
       "test_distance          8.712924   19.762664    8.679669    33.495670   \n",
       "test_r2                0.991028    0.964329    0.991226     0.899752   \n",
       "\n",
       "               PassiveAggressiveRegressor          SVR  DecisionTreeRegressor  \\\n",
       "fit_time                         0.783983   348.231184               1.469077   \n",
       "score_time                       0.081812   154.376145               0.082450   \n",
       "test_mae                       158.575941    63.791369               2.782402   \n",
       "test_mse                     36130.775816  6159.190611              56.103192   \n",
       "test_distance                  161.781433    65.080863               2.838646   \n",
       "test_r2                         -1.342505     0.602990               0.996383   \n",
       "\n",
       "               KNeighborsRegressor  GaussianProcessRegressor  \\\n",
       "fit_time                  1.982650              3.064316e+02   \n",
       "score_time               31.751880              9.619912e+01   \n",
       "test_mae                  3.806230              3.801109e+03   \n",
       "test_mse                 50.665884              2.235548e+07   \n",
       "test_distance             3.883170              3.877946e+03   \n",
       "test_r2                   0.996740             -1.441417e+03   \n",
       "\n",
       "               AdaBoostRegressor  GradientBoostingRegressor  \\\n",
       "fit_time               83.009644                  38.564748   \n",
       "score_time              0.443865                   0.166302   \n",
       "test_mae                2.061294                   8.871777   \n",
       "test_mse               24.625936                 139.781715   \n",
       "test_distance           2.102961                   9.051113   \n",
       "test_r2                 0.998411                   0.990982   \n",
       "\n",
       "               RandomForestRegressor  ExtraTreesRegressor  BaggingRegressor  \n",
       "fit_time                    8.239344             9.545212          9.469980  \n",
       "score_time                  0.121974             0.135257          0.342275  \n",
       "test_mae                    2.764245             2.540180          2.742543  \n",
       "test_mse                   30.824116            31.411138         30.021256  \n",
       "test_distance               2.820122             2.591528          2.797981  \n",
       "test_r2                     0.998014             0.997974          0.998066  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# ===\n",
    "'mae': make_scorer(mae),# 'mean_absolute_error',\n",
    "'mse': make_scorer(mse),#'mean_squared_error',\n",
    "'distance': make_scorer(distance2d),\n",
    "'r2': 'r2'\n",
    "# ===\n",
    "'nbaccuracy' : make_scorer(accert),\n",
    "'accuracy': 'accuracy'\n",
    "# ===   \n",
    "\"\"\"\n",
    "dfLongitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RandomizedSearch: Probando con un modelo de Clasificación, los demás se probarán en Servidor__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "estimadorDictionary = getClassifierModels(includeEnsambled=True)\n",
    "hypSwitcher = HyperparameterSwitcher()\n",
    "process = 'randomized'\n",
    "n_iteraciones = 2\n",
    "idModeloPrueba = 7\n",
    "\n",
    "result = {}\n",
    "modelName = getClassifierNames(includeEnsambled=True)[idModeloPrueba]\n",
    "    \n",
    "estimador = estimadorDictionary[modelName]\n",
    "parametros = hypSwitcher.getHyperparameters(modelName)(isDummy=False)\n",
    "random_search = RandomizedSearchCV(estimador, param_distributions=parametros, \n",
    "                                   n_iter=n_iteraciones, cv=kf, scoring=\"accuracy\", \n",
    "                                   return_train_score=False, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train.FLOOR)\n",
    "result[modelName] = random_search.cv_results_\n",
    "\n",
    "df1 = pd.DataFrame(np.array([result[modelName]['mean_test_score'], result[modelName]['std_test_score'],\n",
    "                             result[modelName]['mean_fit_time'], result[modelName]['std_fit_time'],\n",
    "                             result[modelName]['mean_score_time'], result[modelName]['std_score_time']\n",
    "                            ]).T, columns = ['Accuracy', 'stdAccuracy', 'FitTime', 'stdFitTime', 'ScoreTime', 'stdScoreTime'])\n",
    "df2 = pd.DataFrame(result[modelName]['params'])\n",
    "dff = pd.concat([df1,df2], axis=1).sort_values(['Accuracy', 'FitTime'], ascending=[False, True])\n",
    "dff.to_csv(\"result/\" + process + \"/\" + modelName + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ExhaustiveSearch: Probando con un modelo de Clasificación, los demás se probarán en Servidor__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "process = 'exhaustive'\n",
    "n_iteraciones = 2\n",
    "\n",
    "result = {}\n",
    "modelName = getClassifierNames(includeEnsambled=True)[idModeloPrueba]\n",
    "    \n",
    "estimador = estimadorDictionary[modelName]\n",
    "parametros = hypSwitcher.getHyperparameters(modelName)(isDummy=False)\n",
    "grid_search = GridSearchCV(estimador, param_grid=parametros, \n",
    "                                   cv=kf, scoring=\"accuracy\", \n",
    "                                   return_train_score=False, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train.FLOOR)\n",
    "result[modelName] = grid_search.cv_results_\n",
    "\n",
    "df1 = pd.DataFrame(np.array([result[modelName]['mean_test_score'], result[modelName]['std_test_score'],\n",
    "                             result[modelName]['mean_fit_time'], result[modelName]['std_fit_time'],\n",
    "                             result[modelName]['mean_score_time'], result[modelName]['std_score_time']\n",
    "                            ]).T, columns = ['Accuracy', 'stdAccuracy', 'FitTime', 'stdFitTime', 'ScoreTime', 'stdScoreTime'])\n",
    "df2 = pd.DataFrame(result[modelName]['params'])\n",
    "dff = pd.concat([df1,df2], axis=1).sort_values(['Accuracy', 'FitTime'], ascending=[False, True])\n",
    "dff.to_csv(\"result/\" + process + \"/\" + modelName + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EdasSearch: Probando con un modelo de Clasificación, los demás se probarán en Servidor__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indice\tAccuracy class_weight criterion\n",
      "0\t0.962077         None   entropy\n",
      "1\t0.955255         None      gini\n",
      "2\t0.962077         None   entropy\n"
     ]
    }
   ],
   "source": [
    "idModeloPrueba = 7\n",
    "#hypSwitcher = HyperparameterSwitcher()\n",
    "process = 'edas'\n",
    "estimadorDictionary = getClassifierModels(includeEnsambled=True)\n",
    "modelName = getClassifierNames(includeEnsambled=True)[idModeloPrueba]\n",
    "estimador = estimadorDictionary[modelName]\n",
    "parametros = hypSwitcher.getHyperparameters(modelName)(isDummy=False)\n",
    "\n",
    "gm = GeneralMethods(estimador, X_train, y_train.FLOOR, seed=seed) ## manage drop duplicates in sample generation\n",
    "test = EdasHyperparameterSearch(\n",
    "    gm, parametros, estimador, iterations=2, sample_size=2, select_ratio=0.5, debug=True) # sample_size*select_ratio>=1\n",
    "test.run()\n",
    "dff = pd.DataFrame(list(test.resultados)).sort_values(['Accuracy'], ascending=False).reset_index(drop=True)\n",
    "dff.to_csv(\"result/\" + process + \"/\" + modelName + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EasSearch: Probando con un modelo de Clasificación, los demás se probarán en Servidor__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipos: [1, 1], rangos: [1, 1]\n",
      "--- Evolve in 4 possible combinations ---\n",
      "gen\tnevals\tavg     \tmin     \tmax     \tstd       \n",
      "0  \t3     \t0.959967\t0.958082\t0.962077\t0.00163875\n",
      "1  \t2     \t0.960746\t0.958082\t0.962077\t0.0018833 \n",
      "2  \t2     \t0.962077\t0.962077\t0.962077\t1.11022e-16\n",
      "Best individual is: {'criterion': 'entropy', 'class_weight': None}\n",
      "with fitness: 0.9620774431468961\n"
     ]
    }
   ],
   "source": [
    "from lib.easSearch import GeneticSearchCV\n",
    "idModeloPrueba = 7\n",
    "estimadorDictionary = getClassifierModels(includeEnsambled=True)\n",
    "hypSwitcher = HyperparameterSwitcher()\n",
    "process = 'eas'\n",
    "\n",
    "idModeloPrueba = 7\n",
    "result = {}\n",
    "modelName = getClassifierNames(includeEnsambled=True)[idModeloPrueba]\n",
    "estimador = estimadorDictionary[modelName]\n",
    "parametros = hypSwitcher.getHyperparameters(modelName)(isDummy=False)\n",
    "\n",
    "gs2 = GeneticSearchCV(estimador, parametros, cv=kf, n_jobs=4, verbose=1, scoring='accuracy', refit=False\n",
    "                     , generations_number=2, population_size=3)\n",
    "result = gs2.fit(X_train, y_train.FLOOR)\n",
    "dff = pd.DataFrame(list(gs2.result_cache)).sort_values(['Accuracy'], ascending=False).reset_index(drop=True)\n",
    "dff.to_csv(\"result/\" + process + \"/\" + modelName + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'criterion': 'entropy',\n",
       "  'class_weight': None,\n",
       "  'Accuracy': 0.9620774431468961,\n",
       "  'stdAccuracy': 0.005414661214317645,\n",
       "  'Runtime': 2.373400092124939},\n",
       " {'criterion': 'gini',\n",
       "  'class_weight': 'balanced',\n",
       "  'Accuracy': 0.9597418561770128,\n",
       "  'stdAccuracy': 0.0066779850547021204,\n",
       "  'Runtime': 2.697361779212952},\n",
       " {'criterion': 'gini',\n",
       "  'class_weight': None,\n",
       "  'Accuracy': 0.9552550706822374,\n",
       "  'stdAccuracy': 0.0030706812530539177,\n",
       "  'Runtime': 2.5444828271865845},\n",
       " {'criterion': 'gini',\n",
       "  'class_weight': None,\n",
       "  'Accuracy': 0.9552550706822374,\n",
       "  'stdAccuracy': 0.0030706812530539177,\n",
       "  'Runtime': 2.5315757513046266}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test.resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbTest.graphicMap3D(columns = [\"LATITUDE\", \"LONGITUDE\", \"FLOOR\"], filename=\"buildingsMap3dTest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./lib/\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from lib.Methods import GeneralMethods\n",
    "from lib.edasSearch import EdasHyperparameterSearch\n",
    "from lib.Hiperparametros import HyperparameterSwitcher\n",
    "from lib.ImportacionModelos import getClassifierNames\n",
    "from lib.ImportacionModelos import getClassifierModels\n",
    "from lib.ImportacionModelos import getRegressorNames\n",
    "from lib.ImportacionModelos import getRegressorModels\n",
    "from lib.graphicGenerator import GraphicBuilder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.base import is_classifier\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics as scoreMetrics\n",
    "# import geopy.distance\n",
    "from functools import reduce\n",
    "\n",
    "def accert(y_true, y_pred): \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return (cm.diagonal()/cm.sum(0)).mean()\n",
    "\"\"\"\n",
    "\n",
    "_meanLat = 39.9926853\n",
    "_meanLon = -0.0673033\n",
    "_minLongitude = -7705\n",
    "_maxLongitude = -7290\n",
    "_minLatitude = 4864735\n",
    "_maxLatitude = 4865023\n",
    "_maxLatitudeGPS = 39.993720\n",
    "_maxLongitudeGPS = -0.069254\n",
    "_minLatitudeGPS = 39.991626\n",
    "_minLongitudeGPS = -0.065425\n",
    "\n",
    "def longitudeToGPS(x):\n",
    "    return (_maxLongitudeGPS - _minLongitudeGPS) * (x - _minLongitude) / (_maxLongitude - _minLongitude) + _minLongitudeGPS\n",
    "\n",
    "def latitudeToGPS(x):\n",
    "    return (_maxLatitudeGPS - _minLatitudeGPS) * (x - _minLatitude) / (_maxLatitude - _minLatitude) + _minLatitudeGPS\n",
    "\n",
    "def latitudeListDistance(y_true, y_pred):\n",
    "    return list(map(lambda yt,yp : geopy.distance.vincenty((_meanLon, yt),(_meanLon, yp)).m , latitudeToGPS(y_true), latitudeToGPS(y_pred)))\n",
    "\n",
    "def longitudeListDistance(y_true, y_pred):\n",
    "    return list(map(lambda yt,yp : geopy.distance.vincenty((yt, _meanLat),(yp, _meanLat)).m , longitudeToGPS(y_true), longitudeToGPS(y_pred)))    \n",
    "\n",
    "def distance2d(y_true, y_pred):\n",
    "    ldis = []\n",
    "    if ((y_true>0).sum()>0):\n",
    "        #ldis = list(map(lambda yt,yp : geopy.distance.vincenty((_meanLon, yt),(_meanLon, yp)).m , latitudeToGPS(y_true), latitudeToGPS(y_pred)))\n",
    "        ldis = latitudeListDistance(y_true, y_pred)\n",
    "    else:\n",
    "        #ldis = list(map(lambda yt,yp : geopy.distance.vincenty((yt, _meanLat),(yp, _meanLat)).m , longitudeToGPS(y_true), longitudeToGPS(y_pred)))\n",
    "        ldis = longitudeListDistance(y_true, y_pred)\n",
    "    return reduce(lambda x,y: x+y, ldis) / len(ldis)\n",
    "\"\"\"\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return scoreMetrics.mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "def mae(y_true, y_pred):\n",
    "    return scoreMetrics.mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def score_metrics(estimator, binary=False, params = {}):\n",
    "    score_ = {}\n",
    "    if (is_classifier(estimator)):\n",
    "        if (binary):\n",
    "            score_['average_precision'] = 'average_precision_weighted'\n",
    "            score_['precision'] = 'precision'\n",
    "            score_['recall'] = 'recall'\n",
    "            score_['balanced_accuracy'] = 'balanced_accuracy'\n",
    "            score_['roc_auc'] = 'roc_auc'\n",
    "        score_['nbaccuracy'] = make_scorer(accert)\n",
    "        score_['accuracy'] = 'accuracy'\n",
    "    else:\n",
    "        score_['mae'] = make_scorer(mae) # 'mean_absolute_error',\n",
    "        score_['mse'] = make_scorer(mse) # 'mean_squared_error',\n",
    "        # score['distance'] = make_scorer(distance2d)\n",
    "        score_['r2'] = 'r2'\n",
    "    return score_\n",
    "\n",
    "def cv_fold(estimator, n_splits=10, test_size=0.2, random_state=7):\n",
    "    if (is_classifier(estimator)):\n",
    "        return ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "    else:\n",
    "        return StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def clean_dataframe(df, input_columns, target_column, list_val):\n",
    "    x_train = df_train[input_columns]\n",
    "    y_train = df_train[target_column]\n",
    "    if (list_val[0]):\n",
    "        x_train = x_train.apply(lambda x: 100-x, axis=1)\n",
    "    if (list_val[1]):\n",
    "        x_train = preprocessing.scale(x_train)\n",
    "    if (list_val[2]):\n",
    "        x_train = preprocessing.normalize(x_train)\n",
    "    return x_train, y_train\n",
    "\n",
    "df_train = pd.read_csv(\"data/UJIndoorLoc_trainingData.csv\")\n",
    "df_test = pd.read_csv(\"data/UJIndoorLoc_validationData.csv\")\n",
    "\n",
    "wifi_columns = df_train.columns[:520]\n",
    "target_column = ['BUILDINGID', 'FLOOR', 'LATITUDE', 'LONGITUDE']\n",
    "building = 0\n",
    "floor = 1\n",
    "latitude = 2\n",
    "longitude = 3\n",
    "seed = 7\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "classifier_model = LogisticRegression()\n",
    "regression_model = LinearRegression()\n",
    "\n",
    "## train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
    "## test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "estimator = regression_model\n",
    "kf = cv_fold(estimator)\n",
    "\n",
    "result_list = []\n",
    "iteration = [[i%2, int(i/2)%2, int(i/4)] for i in range(8)]\n",
    "\"\"\"\n",
    "for item in iteration:\n",
    "    x, y = clean_dataframe(df_train, wifi_columns, target_column[building], item)\n",
    "    result = cross_validate(estimator, x, y, scoring=score_metrics(estimator), cv=kf, return_train_score=False)\n",
    "    display(pd.DataFrame(result).mean().astype(str) + \" +/- \" + pd.DataFrame(result).std().astype(str))\n",
    "    result_list.append(result)\n",
    "\"\"\"\n",
    "def build_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation=tf.nn.relu, input_shape=[input_shape]),\n",
    "        layers.Dense(64, activation=tf.nn.relu),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "    model = build_model(len(x_train.keys())) #520\n",
    "\n",
    "\n",
    "#print(result_list)\n",
    "#pd.DataFrame(result_list).T #.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
