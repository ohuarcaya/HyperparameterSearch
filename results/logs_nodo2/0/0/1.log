/usr/lib/python3.4/site-packages/deap/tools/_hypervolume/pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.
  "module. Expect this to be very slow.", ImportWarning)
multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/_parallel_backends.py", line 344, in __call__
    return self.func(*args, **kwargs)
  File "/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py", line 131, in <listcomp>
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "/usr/lib64/python3.4/site-packages/sklearn/model_selection/_validation.py", line 227, in _fit_and_score
    estimator.set_params(**parameters)
  File "/usr/lib64/python3.4/site-packages/sklearn/linear_model/stochastic_gradient.py", line 84, in set_params
    self._validate_params()
  File "/usr/lib64/python3.4/site-packages/sklearn/linear_model/stochastic_gradient.py", line 103, in _validate_params
    raise ValueError("eta0 must be > 0")
ValueError: eta0 must be > 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib64/python3.4/multiprocessing/pool.py", line 119, in worker
    result = (True, func(*args, **kwds))
  File "/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/_parallel_backends.py", line 353, in __call__
    raise TransportableException(text, e_type)
sklearn.externals.joblib.my_exceptions.TransportableException: TransportableException
___________________________________________________________________________
ValueError                                         Tue Dec 11 03:39:13 2018
PID: 3238                                    Python 3.4.9: /usr/bin/python3
...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_and_score>, (SGDClassifier(alpha=0.0001, average=False, class...
       shuffle=True, verbose=0, warm_start=True),        WAP001  WAP002  WAP003  WAP004  WAP005   ...01             503  

[16270 rows x 1041 columns], 6775     0
7566     1
17990    1
1180     4
1050...1      3
Name: FLOOR, Length: 16270, dtype: int64, make_scorer(accuracy_score), array([ 1627,  1628,  1629, ..., 16267, 16268, 16269]), array([   0,    1,    2, ..., 1624, 1625, 1626]), 0, {'alpha': 0.0001, 'class_weight': None, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'elasticnet', 'warm_start': True}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': False})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_and_score>
        args = (SGDClassifier(alpha=0.0001, average=False, class...
       shuffle=True, verbose=0, warm_start=True),        WAP001  WAP002  WAP003  WAP004  WAP005   ...01             503  

[16270 rows x 1041 columns], 6775     0
7566     1
17990    1
1180     4
1050...1      3
Name: FLOOR, Length: 16270, dtype: int64, make_scorer(accuracy_score), array([ 1627,  1628,  1629, ..., 16267, 16268, 16269]), array([   0,    1,    2, ..., 1624, 1625, 1626]), 0, {'alpha': 0.0001, 'class_weight': None, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'elasticnet', 'warm_start': True})
        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': False}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=SGDClassifier(alpha=0.0001, average=False, class...
       shuffle=True, verbose=0, warm_start=True), X=       WAP001  WAP002  WAP003  WAP004  WAP005   ...01             503  

[16270 rows x 1041 columns], y=6775     0
7566     1
17990    1
1180     4
1050...1      3
Name: FLOOR, Length: 16270, dtype: int64, scorer=make_scorer(accuracy_score), train=array([ 1627,  1628,  1629, ..., 16267, 16268, 16269]), test=array([   0,    1,    2, ..., 1624, 1625, 1626]), verbose=0, parameters={'alpha': 0.0001, 'class_weight': None, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'elasticnet', 'warm_start': True}, fit_params={}, return_train_score=False, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')
    222     fit_params = fit_params if fit_params is not None else {}
    223     fit_params = dict([(k, _index_param_value(X, v, train))
    224                       for k, v in fit_params.items()])
    225 
    226     if parameters is not None:
--> 227         estimator.set_params(**parameters)
        estimator.set_params = <bound method SGDClassifier.set_params of SGDCla...       shuffle=True, verbose=0, warm_start=True)>
        parameters = {'alpha': 0.0001, 'class_weight': None, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'elasticnet', 'warm_start': True}
    228 
    229     start_time = time.time()
    230 
    231     X_train, y_train = _safe_split(estimator, X, y, train)

...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/linear_model/stochastic_gradient.py in set_params(self=SGDClassifier(alpha=0.0001, average=False, class...
       shuffle=True, verbose=0, warm_start=True), *args=(), **kwargs={'alpha': 0.0001, 'class_weight': None, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'elasticnet', 'warm_start': True})
     79         # must not be int (e.g. if ``learning_rate=='optimal'``)
     80         self.t_ = None
     81 
     82     def set_params(self, *args, **kwargs):
     83         super(BaseSGD, self).set_params(*args, **kwargs)
---> 84         self._validate_params()
        self._validate_params = <bound method SGDClassifier._validate_params of ...       shuffle=True, verbose=0, warm_start=True)>
     85         return self
     86 
     87     @abstractmethod
     88     def fit(self, X, y):

...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/linear_model/stochastic_gradient.py in _validate_params(self=SGDClassifier(alpha=0.0001, average=False, class...
       shuffle=True, verbose=0, warm_start=True))
     98             raise ValueError("l1_ratio must be in [0, 1]")
     99         if self.alpha < 0.0:
    100             raise ValueError("alpha must be >= 0")
    101         if self.learning_rate in ("constant", "invscaling"):
    102             if self.eta0 <= 0.0:
--> 103                 raise ValueError("eta0 must be > 0")
    104         if self.learning_rate == "optimal" and self.alpha == 0:
    105             raise ValueError("alpha must be > 0 since "
    106                              "learning_rate is 'optimal'. alpha is used "
    107                              "to compute the optimal learning rate.")

ValueError: eta0 must be > 0
___________________________________________________________________________
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py", line 682, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "/usr/lib64/python3.4/multiprocessing/pool.py", line 608, in get
    raise self._value
sklearn.externals.joblib.my_exceptions.TransportableException: TransportableException
___________________________________________________________________________
ValueError                                         Tue Dec 11 03:39:13 2018
PID: 3238                                    Python 3.4.9: /usr/bin/python3
...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_and_score>, (SGDClassifier(alpha=0.0001, average=False, class...
       shuffle=True, verbose=0, warm_start=True),        WAP001  WAP002  WAP003  WAP004  WAP005   ...01             503  

[16270 rows x 1041 columns], 6775     0
7566     1
17990    1
1180     4
1050...1      3
Name: FLOOR, Length: 16270, dtype: int64, make_scorer(accuracy_score), array([ 1627,  1628,  1629, ..., 16267, 16268, 16269]), array([   0,    1,    2, ..., 1624, 1625, 1626]), 0, {'alpha': 0.0001, 'class_weight': None, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'elasticnet', 'warm_start': True}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': False})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_and_score>
        args = (SGDClassifier(alpha=0.0001, average=False, class...
       shuffle=True, verbose=0, warm_start=True),        WAP001  WAP002  WAP003  WAP004  WAP005   ...01             503  

[16270 rows x 1041 columns], 6775     0
7566     1
17990    1
1180     4
1050...1      3
Name: FLOOR, Length: 16270, dtype: int64, make_scorer(accuracy_score), array([ 1627,  1628,  1629, ..., 16267, 16268, 16269]), array([   0,    1,    2, ..., 1624, 1625, 1626]), 0, {'alpha': 0.0001, 'class_weight': None, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'elasticnet', 'warm_start': True})
        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': False}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=SGDClassifier(alpha=0.0001, average=False, class...
       shuffle=True, verbose=0, warm_start=True), X=       WAP001  WAP002  WAP003  WAP004  WAP005   ...01             503  

[16270 rows x 1041 columns], y=6775     0
7566     1
17990    1
1180     4
1050...1      3
Name: FLOOR, Length: 16270, dtype: int64, scorer=make_scorer(accuracy_score), train=array([ 1627,  1628,  1629, ..., 16267, 16268, 16269]), test=array([   0,    1,    2, ..., 1624, 1625, 1626]), verbose=0, parameters={'alpha': 0.0001, 'class_weight': None, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'elasticnet', 'warm_start': True}, fit_params={}, return_train_score=False, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')
    222     fit_params = fit_params if fit_params is not None else {}
    223     fit_params = dict([(k, _index_param_value(X, v, train))
    224                       for k, v in fit_params.items()])
    225 
    226     if parameters is not None:
--> 227         estimator.set_params(**parameters)
        estimator.set_params = <bound method SGDClassifier.set_params of SGDCla...       shuffle=True, verbose=0, warm_start=True)>
        parameters = {'alpha': 0.0001, 'class_weight': None, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'elasticnet', 'warm_start': True}
    228 
    229     start_time = time.time()
    230 
    231     X_train, y_train = _safe_split(estimator, X, y, train)

...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/linear_model/stochastic_gradient.py in set_params(self=SGDClassifier(alpha=0.0001, average=False, class...
       shuffle=True, verbose=0, warm_start=True), *args=(), **kwargs={'alpha': 0.0001, 'class_weight': None, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'elasticnet', 'warm_start': True})
     79         # must not be int (e.g. if ``learning_rate=='optimal'``)
     80         self.t_ = None
     81 
     82     def set_params(self, *args, **kwargs):
     83         super(BaseSGD, self).set_params(*args, **kwargs)
---> 84         self._validate_params()
        self._validate_params = <bound method SGDClassifier._validate_params of ...       shuffle=True, verbose=0, warm_start=True)>
     85         return self
     86 
     87     @abstractmethod
     88     def fit(self, X, y):

...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/linear_model/stochastic_gradient.py in _validate_params(self=SGDClassifier(alpha=0.0001, average=False, class...
       shuffle=True, verbose=0, warm_start=True))
     98             raise ValueError("l1_ratio must be in [0, 1]")
     99         if self.alpha < 0.0:
    100             raise ValueError("alpha must be >= 0")
    101         if self.learning_rate in ("constant", "invscaling"):
    102             if self.eta0 <= 0.0:
--> 103                 raise ValueError("eta0 must be > 0")
    104         if self.learning_rate == "optimal" and self.alpha == 0:
    105             raise ValueError("alpha must be > 0 since "
    106                              "learning_rate is 'optimal'. alpha is used "
    107                              "to compute the optimal learning rate.")

ValueError: eta0 must be > 0
___________________________________________________________________________

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 58, in <module>
    ev.fit(scoring='accuracy', n_jobs=cpu_count(), kargs=searchParams)
  File "/home/Manuel.Castillo/pruebasOHC/lib/ProcessManager.py", line 62, in fit
    rscv.fit(self.X, self.y)
  File "/usr/lib64/python3.4/site-packages/sklearn/model_selection/_search.py", line 1190, in fit
    return self._fit(X, y, groups, sampled_params)
  File "/usr/lib64/python3.4/site-packages/sklearn/model_selection/_search.py", line 564, in _fit
    for parameters in parameter_iterable
  File "/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py", line 768, in __call__
    self.retrieve()
  File "/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py", line 719, in retrieve
    raise exception
sklearn.externals.joblib.my_exceptions.JoblibValueError: JoblibValueError
___________________________________________________________________________
Multiprocessing exception:
...........................................................................
/home/Manuel.Castillo/pruebasOHC/main.py in <module>()
     53 """
     54 ev = Evaluator(X_train, y_train[y_column])
     55 ev.setEstimador(estimador)
     56 ev.setParams(parametros)
     57 ev.setTypeSearch(process)
---> 58 ev.fit(scoring='accuracy', n_jobs=cpu_count(), kargs=searchParams)
     59 # Guardar Modelo en formato csv
     60 ev.saveDataFrame(modelName + y_column)
     61 
     62 

...........................................................................
/home/Manuel.Castillo/pruebasOHC/lib/ProcessManager.py in fit(self=<lib.ProcessManager.Evaluator object>, scoring='accuracy', n_jobs=16, kargs={'elit': 5, 'ngen': 10, 'pelit': 0.3, 'psize': 80})
     57             self.dff = pd.concat([df1,df2], axis=1).sort_values(['Accuracy', 'FitTime'], ascending=[False, True])
     58         if (self.type == 'randomized'):
     59             rscv = RandomizedSearchCV(self.estimador, param_distributions=self.params, 
     60                                 n_iter=generaciones, cv=self.kf, scoring="accuracy", 
     61                                 return_train_score=False, n_jobs=n_jobs)
---> 62             rscv.fit(self.X, self.y)
        rscv.fit = <bound method RandomizedSearchCV.fit of Randomiz...rain_score=False, scoring='accuracy', verbose=0)>
        self.X =        WAP001  WAP002  WAP003  WAP004  WAP005   ...01             503  

[16270 rows x 1041 columns]
        self.y = 6775     0
7566     1
17990    1
1180     4
1050...1      3
Name: FLOOR, Length: 16270, dtype: int64
     63             rscv.cv_results_ = rscv.cv_results_
     64             df1 = pd.DataFrame(np.array([rscv.cv_results_['mean_test_score'], rscv.cv_results_['std_test_score'],
     65                                         rscv.cv_results_['mean_fit_time'], rscv.cv_results_['std_fit_time'],
     66                                         rscv.cv_results_['mean_score_time'], rscv.cv_results_['std_score_time']

...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/model_selection/_search.py in fit(self=RandomizedSearchCV(cv=KFold(n_splits=10, random_...train_score=False, scoring='accuracy', verbose=0), X=       WAP001  WAP002  WAP003  WAP004  WAP005   ...01             503  

[16270 rows x 1041 columns], y=6775     0
7566     1
17990    1
1180     4
1050...1      3
Name: FLOOR, Length: 16270, dtype: int64, groups=None)
   1185             train/test set.
   1186         """
   1187         sampled_params = ParameterSampler(self.param_distributions,
   1188                                           self.n_iter,
   1189                                           random_state=self.random_state)
-> 1190         return self._fit(X, y, groups, sampled_params)
        self._fit = <bound method RandomizedSearchCV._fit of Randomi...rain_score=False, scoring='accuracy', verbose=0)>
        X =        WAP001  WAP002  WAP003  WAP004  WAP005   ...01             503  

[16270 rows x 1041 columns]
        y = 6775     0
7566     1
17990    1
1180     4
1050...1      3
Name: FLOOR, Length: 16270, dtype: int64
        groups = None
        sampled_params = <sklearn.model_selection._search.ParameterSampler object>
   1191 
   1192 
   1193 
   1194 

...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/model_selection/_search.py in _fit(self=RandomizedSearchCV(cv=KFold(n_splits=10, random_...train_score=False, scoring='accuracy', verbose=0), X=       WAP001  WAP002  WAP003  WAP004  WAP005   ...01             503  

[16270 rows x 1041 columns], y=6775     0
7566     1
17990    1
1180     4
1050...1      3
Name: FLOOR, Length: 16270, dtype: int64, groups=None, parameter_iterable=<sklearn.model_selection._search.ParameterSampler object>)
    559                                   fit_params=self.fit_params,
    560                                   return_train_score=self.return_train_score,
    561                                   return_n_test_samples=True,
    562                                   return_times=True, return_parameters=True,
    563                                   error_score=self.error_score)
--> 564           for parameters in parameter_iterable
        parameters = undefined
        parameter_iterable = <sklearn.model_selection._search.ParameterSampler object>
    565           for train, test in cv_iter)
    566 
    567         # if one choose to see train score, "out" will contain train score info
    568         if self.return_train_score:

...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=16), iterable=<generator object <genexpr>>)
    763             if pre_dispatch == "all" or n_jobs == 1:
    764                 # The iterable was consumed all at once by the above for loop.
    765                 # No need to wait for async callbacks to trigger to
    766                 # consumption.
    767                 self._iterating = False
--> 768             self.retrieve()
        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=16)>
    769             # Make sure that we get a last message telling us we are done
    770             elapsed_time = time.time() - self._start_time
    771             self._print('Done %3i out of %3i | elapsed: %s finished',
    772                         (len(self._output), len(self._output),

---------------------------------------------------------------------------
Sub-process traceback:
---------------------------------------------------------------------------
ValueError                                         Tue Dec 11 03:39:13 2018
PID: 3238                                    Python 3.4.9: /usr/bin/python3
...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_and_score>, (SGDClassifier(alpha=0.0001, average=False, class...
       shuffle=True, verbose=0, warm_start=True),        WAP001  WAP002  WAP003  WAP004  WAP005   ...01             503  

[16270 rows x 1041 columns], 6775     0
7566     1
17990    1
1180     4
1050...1      3
Name: FLOOR, Length: 16270, dtype: int64, make_scorer(accuracy_score), array([ 1627,  1628,  1629, ..., 16267, 16268, 16269]), array([   0,    1,    2, ..., 1624, 1625, 1626]), 0, {'alpha': 0.0001, 'class_weight': None, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'elasticnet', 'warm_start': True}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': False})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_and_score>
        args = (SGDClassifier(alpha=0.0001, average=False, class...
       shuffle=True, verbose=0, warm_start=True),        WAP001  WAP002  WAP003  WAP004  WAP005   ...01             503  

[16270 rows x 1041 columns], 6775     0
7566     1
17990    1
1180     4
1050...1      3
Name: FLOOR, Length: 16270, dtype: int64, make_scorer(accuracy_score), array([ 1627,  1628,  1629, ..., 16267, 16268, 16269]), array([   0,    1,    2, ..., 1624, 1625, 1626]), 0, {'alpha': 0.0001, 'class_weight': None, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'elasticnet', 'warm_start': True})
        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': False}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=SGDClassifier(alpha=0.0001, average=False, class...
       shuffle=True, verbose=0, warm_start=True), X=       WAP001  WAP002  WAP003  WAP004  WAP005   ...01             503  

[16270 rows x 1041 columns], y=6775     0
7566     1
17990    1
1180     4
1050...1      3
Name: FLOOR, Length: 16270, dtype: int64, scorer=make_scorer(accuracy_score), train=array([ 1627,  1628,  1629, ..., 16267, 16268, 16269]), test=array([   0,    1,    2, ..., 1624, 1625, 1626]), verbose=0, parameters={'alpha': 0.0001, 'class_weight': None, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'elasticnet', 'warm_start': True}, fit_params={}, return_train_score=False, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')
    222     fit_params = fit_params if fit_params is not None else {}
    223     fit_params = dict([(k, _index_param_value(X, v, train))
    224                       for k, v in fit_params.items()])
    225 
    226     if parameters is not None:
--> 227         estimator.set_params(**parameters)
        estimator.set_params = <bound method SGDClassifier.set_params of SGDCla...       shuffle=True, verbose=0, warm_start=True)>
        parameters = {'alpha': 0.0001, 'class_weight': None, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'elasticnet', 'warm_start': True}
    228 
    229     start_time = time.time()
    230 
    231     X_train, y_train = _safe_split(estimator, X, y, train)

...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/linear_model/stochastic_gradient.py in set_params(self=SGDClassifier(alpha=0.0001, average=False, class...
       shuffle=True, verbose=0, warm_start=True), *args=(), **kwargs={'alpha': 0.0001, 'class_weight': None, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'elasticnet', 'warm_start': True})
     79         # must not be int (e.g. if ``learning_rate=='optimal'``)
     80         self.t_ = None
     81 
     82     def set_params(self, *args, **kwargs):
     83         super(BaseSGD, self).set_params(*args, **kwargs)
---> 84         self._validate_params()
        self._validate_params = <bound method SGDClassifier._validate_params of ...       shuffle=True, verbose=0, warm_start=True)>
     85         return self
     86 
     87     @abstractmethod
     88     def fit(self, X, y):

...........................................................................
/usr/lib64/python3.4/site-packages/sklearn/linear_model/stochastic_gradient.py in _validate_params(self=SGDClassifier(alpha=0.0001, average=False, class...
       shuffle=True, verbose=0, warm_start=True))
     98             raise ValueError("l1_ratio must be in [0, 1]")
     99         if self.alpha < 0.0:
    100             raise ValueError("alpha must be >= 0")
    101         if self.learning_rate in ("constant", "invscaling"):
    102             if self.eta0 <= 0.0:
--> 103                 raise ValueError("eta0 must be > 0")
    104         if self.learning_rate == "optimal" and self.alpha == 0:
    105             raise ValueError("alpha must be > 0 since "
    106                              "learning_rate is 'optimal'. alpha is used "
    107                              "to compute the optimal learning rate.")

ValueError: eta0 must be > 0
___________________________________________________________________________
