1551148429.7979674
AdaBoostRegressor
--- 8391.698179244995 seconds ---
BaggingRegressor Failure xxx
JoblibValueError
___________________________________________________________________________
Multiprocessing exception:
...........................................................................
/data/users/oscar/mhSearch/main.py in <module>()
     67     ev.setParams(parametros)
     68     ev.setTypeSearch(process)
     69     n_jobs = cpu_count() # 1
     70     start_time = time.time()
     71     try:
---> 72         ev.fit(scoring='mse', n_jobs=n_jobs, kargs=searchParams)
     73         # except:
     74         #    print("salió un error con el algoritmo %d" %(arg3))
     75         print(start_time)
     76         print(modelName)

...........................................................................
/data/users/oscar/mhSearch/lib/ProcessManager.py in fit(self=<lib.ProcessManager.Evaluator object>, scoring={'approach': make_scorer(distance2d), 'mae': make_scorer(mae), 'mse': make_scorer(mse)}, n_jobs=48, kargs={'elit': 3, 'ngen': 5, 'pelit': 0.5, 'psize': 12})
     49             self.dff = pd.DataFrame(list(edcv.resultados)).sort_values(['Accuracy'], 
     50                     ascending=False).reset_index(drop=True)
     51         if (self.type == 'exhaustive'):
     52             escv = GridSearchCV(self.estimador, param_grid=self.params, cv=self.kf, scoring=scoring, refit=False,
     53                                     return_train_score=False, n_jobs=n_jobs)
---> 54             escv.fit(self.X, self.y)
        escv.fit = <bound method BaseSearchCV.fit of GridSearchCV(c...ch': make_scorer(distance2d)},
       verbose=0)>
        self.X =        WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns]
        self.y = 1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64
     55             if(scoring=="accuracy"):
     56                 df1 = pd.DataFrame(np.array([escv.cv_results_['mean_test_score'], escv.cv_results_['std_test_score'],
     57                                             escv.cv_results_['mean_fit_time'], escv.cv_results_['std_fit_time'],
     58                                             escv.cv_results_['mean_score_time'], escv.cv_results_['std_score_time']

...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=KFold(n_splits=10, random_state=...ach': make_scorer(distance2d)},
       verbose=0), X=       WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns], y=1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64, groups=None, **fit_params={})
    635                                   return_train_score=self.return_train_score,
    636                                   return_n_test_samples=True,
    637                                   return_times=True, return_parameters=False,
    638                                   error_score=self.error_score)
    639           for parameters, (train, test) in product(candidate_params,
--> 640                                                    cv.split(X, y, groups)))
        cv.split = <bound method _BaseKFold.split of KFold(n_splits=10, random_state=9, shuffle=True)>
        X =        WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns]
        y = 1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64
        groups = None
    641 
    642         # if one choose to see train score, "out" will contain train score info
    643         if self.return_train_score:
    644             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,

...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=48), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)
    784             if pre_dispatch == "all" or n_jobs == 1:
    785                 # The iterable was consumed all at once by the above for loop.
    786                 # No need to wait for async callbacks to trigger to
    787                 # consumption.
    788                 self._iterating = False
--> 789             self.retrieve()
        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=48)>
    790             # Make sure that we get a last message telling us we are done
    791             elapsed_time = time.time() - self._start_time
    792             self._print('Done %3i out of %3i | elapsed: %s finished',
    793                         (len(self._output), len(self._output),

---------------------------------------------------------------------------
Sub-process traceback:
---------------------------------------------------------------------------
ValueError                                         Mon Feb 25 23:53:42 2019
PID: 163052                       Python 3.7.2: /data/anaconda3/bin/python3
...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_and_score>, (BaggingRegressor(base_estimator=DecisionTreeRegr...      random_state=7, verbose=0, warm_start=True),        WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns], 1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64, {'approach': make_scorer(distance2d), 'mae': make_scorer(mae), 'mse': make_scorer(mse)}, array([    0,     1,     2, ..., 16323, 16324, 16325]), array([    4,    22,    36, ..., 16294, 16308, 16318]), 0, {'bootstrap': True, 'bootstrap_features': True, 'n_estimators': 5, 'oob_score': True, 'warm_start': True}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': False})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_and_score>
        args = (BaggingRegressor(base_estimator=DecisionTreeRegr...      random_state=7, verbose=0, warm_start=True),        WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns], 1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64, {'approach': make_scorer(distance2d), 'mae': make_scorer(mae), 'mse': make_scorer(mse)}, array([    0,     1,     2, ..., 16323, 16324, 16325]), array([    4,    22,    36, ..., 16294, 16308, 16318]), 0, {'bootstrap': True, 'bootstrap_features': True, 'n_estimators': 5, 'oob_score': True, 'warm_start': True})
        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': False}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=BaggingRegressor(base_estimator=DecisionTreeRegr...      random_state=7, verbose=0, warm_start=True), X=       WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns], y=1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64, scorer={'approach': make_scorer(distance2d), 'mae': make_scorer(mae), 'mse': make_scorer(mse)}, train=array([    0,     1,     2, ..., 16323, 16324, 16325]), test=array([    4,    22,    36, ..., 16294, 16308, 16318]), verbose=0, parameters={'bootstrap': True, 'bootstrap_features': True, 'n_estimators': 5, 'oob_score': True, 'warm_start': True}, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')
    453 
    454     try:
    455         if y_train is None:
    456             estimator.fit(X_train, **fit_params)
    457         else:
--> 458             estimator.fit(X_train, y_train, **fit_params)
        estimator.fit = <bound method BaseBagging.fit of BaggingRegresso...     random_state=7, verbose=0, warm_start=True)>
        X_train =        WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[14693 rows x 1055 columns]
        y_train = 1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 14693, dtype: float64
        fit_params = {}
    459 
    460     except Exception as e:
    461         # Note fit time as time until error
    462         fit_time = time.time() - start_time

...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/bagging.py in fit(self=BaggingRegressor(base_estimator=DecisionTreeRegr...      random_state=7, verbose=0, warm_start=True), X=       WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[14693 rows x 1055 columns], y=1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 14693, dtype: float64, sample_weight=None)
    242         Returns
    243         -------
    244         self : object
    245             Returns self.
    246         """
--> 247         return self._fit(X, y, self.max_samples, sample_weight=sample_weight)
        self._fit = <bound method BaseBagging._fit of BaggingRegress...     random_state=7, verbose=0, warm_start=True)>
        X =        WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[14693 rows x 1055 columns]
        y = 1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 14693, dtype: float64
        self.max_samples = 1.0
        sample_weight = None
    248 
    249     def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):
    250         """Build a Bagging ensemble of estimators from the training
    251            set (X, y).

...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/bagging.py in _fit(self=BaggingRegressor(base_estimator=DecisionTreeRegr...      random_state=7, verbose=0, warm_start=True), X=array([[0., 0., 0., ..., 0., 0., 0.],
       [0...., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]]), y=array([-7512.60416363, -7474.67      , -7359.193... -7398.4282    , -7676.6436    , -7413.9214    ]), max_samples=14693, max_depth=None, sample_weight=None)
    324         if not self.bootstrap and self.oob_score:
    325             raise ValueError("Out of bag estimation only available"
    326                              " if bootstrap=True")
    327 
    328         if self.warm_start and self.oob_score:
--> 329             raise ValueError("Out of bag estimate only available"
    330                              " if warm_start=False")
    331 
    332         if hasattr(self, "oob_score_") and self.warm_start:
    333             del self.oob_score_

ValueError: Out of bag estimate only available if warm_start=False
___________________________________________________________________________
ExtraTreesRegressor Failure xxx
JoblibValueError
___________________________________________________________________________
Multiprocessing exception:
...........................................................................
/data/users/oscar/mhSearch/main.py in <module>()
     67     ev.setParams(parametros)
     68     ev.setTypeSearch(process)
     69     n_jobs = cpu_count() # 1
     70     start_time = time.time()
     71     try:
---> 72         ev.fit(scoring='mse', n_jobs=n_jobs, kargs=searchParams)
     73         # except:
     74         #    print("salió un error con el algoritmo %d" %(arg3))
     75         print(start_time)
     76         print(modelName)

...........................................................................
/data/users/oscar/mhSearch/lib/ProcessManager.py in fit(self=<lib.ProcessManager.Evaluator object>, scoring={'approach': make_scorer(distance2d), 'mae': make_scorer(mae), 'mse': make_scorer(mse)}, n_jobs=48, kargs={'elit': 3, 'ngen': 7, 'pelit': 0.5, 'psize': 30})
     49             self.dff = pd.DataFrame(list(edcv.resultados)).sort_values(['Accuracy'], 
     50                     ascending=False).reset_index(drop=True)
     51         if (self.type == 'exhaustive'):
     52             escv = GridSearchCV(self.estimador, param_grid=self.params, cv=self.kf, scoring=scoring, refit=False,
     53                                     return_train_score=False, n_jobs=n_jobs)
---> 54             escv.fit(self.X, self.y)
        escv.fit = <bound method BaseSearchCV.fit of GridSearchCV(c...ch': make_scorer(distance2d)},
       verbose=0)>
        self.X =        WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns]
        self.y = 1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64
     55             if(scoring=="accuracy"):
     56                 df1 = pd.DataFrame(np.array([escv.cv_results_['mean_test_score'], escv.cv_results_['std_test_score'],
     57                                             escv.cv_results_['mean_fit_time'], escv.cv_results_['std_fit_time'],
     58                                             escv.cv_results_['mean_score_time'], escv.cv_results_['std_score_time']

...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=KFold(n_splits=10, random_state=...ach': make_scorer(distance2d)},
       verbose=0), X=       WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns], y=1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64, groups=None, **fit_params={})
    635                                   return_train_score=self.return_train_score,
    636                                   return_n_test_samples=True,
    637                                   return_times=True, return_parameters=False,
    638                                   error_score=self.error_score)
    639           for parameters, (train, test) in product(candidate_params,
--> 640                                                    cv.split(X, y, groups)))
        cv.split = <bound method _BaseKFold.split of KFold(n_splits=10, random_state=9, shuffle=True)>
        X =        WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns]
        y = 1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64
        groups = None
    641 
    642         # if one choose to see train score, "out" will contain train score info
    643         if self.return_train_score:
    644             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,

...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=48), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)
    784             if pre_dispatch == "all" or n_jobs == 1:
    785                 # The iterable was consumed all at once by the above for loop.
    786                 # No need to wait for async callbacks to trigger to
    787                 # consumption.
    788                 self._iterating = False
--> 789             self.retrieve()
        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=48)>
    790             # Make sure that we get a last message telling us we are done
    791             elapsed_time = time.time() - self._start_time
    792             self._print('Done %3i out of %3i | elapsed: %s finished',
    793                         (len(self._output), len(self._output),

---------------------------------------------------------------------------
Sub-process traceback:
---------------------------------------------------------------------------
ValueError                                         Tue Feb 26 05:22:29 2019
PID: 163268                       Python 3.7.2: /data/anaconda3/bin/python3
...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_and_score>, (ExtraTreesRegressor(bootstrap=False, criterion='...True, random_state=7, verbose=0, warm_start=True),        WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns], 1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64, {'approach': make_scorer(distance2d), 'mae': make_scorer(mae), 'mse': make_scorer(mse)}, array([    0,     1,     2, ..., 16323, 16324, 16325]), array([    4,    22,    36, ..., 16294, 16308, 16318]), 0, {'bootstrap': False, 'criterion': 'mse', 'max_features': None, 'n_estimators': 10, 'oob_score': True, 'warm_start': True}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': False})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_and_score>
        args = (ExtraTreesRegressor(bootstrap=False, criterion='...True, random_state=7, verbose=0, warm_start=True),        WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns], 1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64, {'approach': make_scorer(distance2d), 'mae': make_scorer(mae), 'mse': make_scorer(mse)}, array([    0,     1,     2, ..., 16323, 16324, 16325]), array([    4,    22,    36, ..., 16294, 16308, 16318]), 0, {'bootstrap': False, 'criterion': 'mse', 'max_features': None, 'n_estimators': 10, 'oob_score': True, 'warm_start': True})
        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': False}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=ExtraTreesRegressor(bootstrap=False, criterion='...True, random_state=7, verbose=0, warm_start=True), X=       WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns], y=1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64, scorer={'approach': make_scorer(distance2d), 'mae': make_scorer(mae), 'mse': make_scorer(mse)}, train=array([    0,     1,     2, ..., 16323, 16324, 16325]), test=array([    4,    22,    36, ..., 16294, 16308, 16318]), verbose=0, parameters={'bootstrap': False, 'criterion': 'mse', 'max_features': None, 'n_estimators': 10, 'oob_score': True, 'warm_start': True}, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')
    453 
    454     try:
    455         if y_train is None:
    456             estimator.fit(X_train, **fit_params)
    457         else:
--> 458             estimator.fit(X_train, y_train, **fit_params)
        estimator.fit = <bound method BaseForest.fit of ExtraTreesRegres...rue, random_state=7, verbose=0, warm_start=True)>
        X_train =        WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[14693 rows x 1055 columns]
        y_train = 1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 14693, dtype: float64
        fit_params = {}
    459 
    460     except Exception as e:
    461         # Note fit time as time until error
    462         fit_time = time.time() - start_time

...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py in fit(self=ExtraTreesRegressor(bootstrap=False, criterion='...True, random_state=7, verbose=0, warm_start=True), X=array([[0., 0., 0., ..., 0., 0., 0.],
       [0....   [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), y=array([[-7512.60416363],
       [-7474.67      ]...      [-7676.6436    ],
       [-7413.9214    ]]), sample_weight=None)
    283 
    284         # Check parameters
    285         self._validate_estimator()
    286 
    287         if not self.bootstrap and self.oob_score:
--> 288             raise ValueError("Out of bag estimation only available"
    289                              " if bootstrap=True")
    290 
    291         random_state = check_random_state(self.random_state)
    292 

ValueError: Out of bag estimation only available if bootstrap=True
___________________________________________________________________________
1551178571.868495
KNeighborsRegressor
--- 337462.18115496635 seconds ---
RandomForestRegressor Failure xxx
JoblibValueError
___________________________________________________________________________
Multiprocessing exception:
...........................................................................
/data/users/oscar/mhSearch/main.py in <module>()
     67     ev.setParams(parametros)
     68     ev.setTypeSearch(process)
     69     n_jobs = cpu_count() # 1
     70     start_time = time.time()
     71     try:
---> 72         ev.fit(scoring='mse', n_jobs=n_jobs, kargs=searchParams)
     73         # except:
     74         #    print("salió un error con el algoritmo %d" %(arg3))
     75         print(start_time)
     76         print(modelName)

...........................................................................
/data/users/oscar/mhSearch/lib/ProcessManager.py in fit(self=<lib.ProcessManager.Evaluator object>, scoring={'approach': make_scorer(distance2d), 'mae': make_scorer(mae), 'mse': make_scorer(mse)}, n_jobs=48, kargs={'elit': 3, 'ngen': 5, 'pelit': 0.5, 'psize': 25})
     49             self.dff = pd.DataFrame(list(edcv.resultados)).sort_values(['Accuracy'], 
     50                     ascending=False).reset_index(drop=True)
     51         if (self.type == 'exhaustive'):
     52             escv = GridSearchCV(self.estimador, param_grid=self.params, cv=self.kf, scoring=scoring, refit=False,
     53                                     return_train_score=False, n_jobs=n_jobs)
---> 54             escv.fit(self.X, self.y)
        escv.fit = <bound method BaseSearchCV.fit of GridSearchCV(c...ch': make_scorer(distance2d)},
       verbose=0)>
        self.X =        WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns]
        self.y = 1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64
     55             if(scoring=="accuracy"):
     56                 df1 = pd.DataFrame(np.array([escv.cv_results_['mean_test_score'], escv.cv_results_['std_test_score'],
     57                                             escv.cv_results_['mean_fit_time'], escv.cv_results_['std_fit_time'],
     58                                             escv.cv_results_['mean_score_time'], escv.cv_results_['std_score_time']

...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=KFold(n_splits=10, random_state=...ach': make_scorer(distance2d)},
       verbose=0), X=       WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns], y=1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64, groups=None, **fit_params={})
    635                                   return_train_score=self.return_train_score,
    636                                   return_n_test_samples=True,
    637                                   return_times=True, return_parameters=False,
    638                                   error_score=self.error_score)
    639           for parameters, (train, test) in product(candidate_params,
--> 640                                                    cv.split(X, y, groups)))
        cv.split = <bound method _BaseKFold.split of KFold(n_splits=10, random_state=9, shuffle=True)>
        X =        WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns]
        y = 1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64
        groups = None
    641 
    642         # if one choose to see train score, "out" will contain train score info
    643         if self.return_train_score:
    644             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,

...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=48), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)
    784             if pre_dispatch == "all" or n_jobs == 1:
    785                 # The iterable was consumed all at once by the above for loop.
    786                 # No need to wait for async callbacks to trigger to
    787                 # consumption.
    788                 self._iterating = False
--> 789             self.retrieve()
        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=48)>
    790             # Make sure that we get a last message telling us we are done
    791             elapsed_time = time.time() - self._start_time
    792             self._print('Done %3i out of %3i | elapsed: %s finished',
    793                         (len(self._output), len(self._output),

---------------------------------------------------------------------------
Sub-process traceback:
---------------------------------------------------------------------------
ValueError                                         Sat Mar  2 07:16:21 2019
PID: 131996                       Python 3.7.2: /data/anaconda3/bin/python3
...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_and_score>, (RandomForestRegressor(bootstrap=False, criterion...True, random_state=7, verbose=0, warm_start=True),        WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns], 1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64, {'approach': make_scorer(distance2d), 'mae': make_scorer(mae), 'mse': make_scorer(mse)}, array([    0,     1,     2, ..., 16323, 16324, 16325]), array([    4,    22,    36, ..., 16294, 16308, 16318]), 0, {'bootstrap': False, 'criterion': 'mse', 'max_features': None, 'n_estimators': 5, 'oob_score': True, 'warm_start': True}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': False})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_and_score>
        args = (RandomForestRegressor(bootstrap=False, criterion...True, random_state=7, verbose=0, warm_start=True),        WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns], 1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64, {'approach': make_scorer(distance2d), 'mae': make_scorer(mae), 'mse': make_scorer(mse)}, array([    0,     1,     2, ..., 16323, 16324, 16325]), array([    4,    22,    36, ..., 16294, 16308, 16318]), 0, {'bootstrap': False, 'criterion': 'mse', 'max_features': None, 'n_estimators': 5, 'oob_score': True, 'warm_start': True})
        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': False}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=RandomForestRegressor(bootstrap=False, criterion...True, random_state=7, verbose=0, warm_start=True), X=       WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[16326 rows x 1055 columns], y=1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 16326, dtype: float64, scorer={'approach': make_scorer(distance2d), 'mae': make_scorer(mae), 'mse': make_scorer(mse)}, train=array([    0,     1,     2, ..., 16323, 16324, 16325]), test=array([    4,    22,    36, ..., 16294, 16308, 16318]), verbose=0, parameters={'bootstrap': False, 'criterion': 'mse', 'max_features': None, 'n_estimators': 5, 'oob_score': True, 'warm_start': True}, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')
    453 
    454     try:
    455         if y_train is None:
    456             estimator.fit(X_train, **fit_params)
    457         else:
--> 458             estimator.fit(X_train, y_train, **fit_params)
        estimator.fit = <bound method BaseForest.fit of RandomForestRegr...rue, random_state=7, verbose=0, warm_start=True)>
        X_train =        WAP001  WAP002  WAP003  WAP004  ...   NEX...    0        0     0

[14693 rows x 1055 columns]
        y_train = 1007    -7512.604164
14408   -7474.670000
15019 ...00
Name: LONGITUDE, Length: 14693, dtype: float64
        fit_params = {}
    459 
    460     except Exception as e:
    461         # Note fit time as time until error
    462         fit_time = time.time() - start_time

...........................................................................
/data/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py in fit(self=RandomForestRegressor(bootstrap=False, criterion...True, random_state=7, verbose=0, warm_start=True), X=array([[0., 0., 0., ..., 0., 0., 0.],
       [0....   [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), y=array([[-7512.60416363],
       [-7474.67      ]...      [-7676.6436    ],
       [-7413.9214    ]]), sample_weight=None)
    283 
    284         # Check parameters
    285         self._validate_estimator()
    286 
    287         if not self.bootstrap and self.oob_score:
--> 288             raise ValueError("Out of bag estimation only available"
    289                              " if bootstrap=True")
    290 
    291         random_state = check_random_state(self.random_state)
    292 

ValueError: Out of bag estimation only available if bootstrap=True
___________________________________________________________________________
